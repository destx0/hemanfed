{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 06:39:58.580634: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-30 06:39:58.580671: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-30 06:39:58.580700: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-30 06:39:58.586931: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-30 06:39:59.325971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives import padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import os\n",
    "import pickle\n",
    "import os\n",
    "import pickle\n",
    "import pickle\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [0 , 1, 2]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train_all = x_train_all.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# # Define the MAML model\n",
    "# class MAML(tf.keras.Model):\n",
    "#     def __init__(self, model):\n",
    "#         super(MAML, self).__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def train_step(self, data):\n",
    "#         x, y = data\n",
    "#         x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "#         y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = self.model(x)\n",
    "#             loss = self.compiled_loss(y, y_pred)\n",
    "#         gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "#         self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "#     def test_step(self, data):\n",
    "#         x, y = data\n",
    "#         x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "#         y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "#         y_pred = self.model(x)\n",
    "#         self.compiled_loss(y, y_pred)\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(MAML, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "        return self.model(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"model\": self.model.get_config()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        model = tf.keras.models.Model.from_config(config[\"model\"])\n",
    "        return cls(model)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "        y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "        y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "        y_pred = self.model(x)\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "# Define the meta-learning parameters\n",
    "num_meta_updates = 10\n",
    "num_inner_updates = 5\n",
    "meta_batch_size = 32\n",
    "inner_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume X is your feature data and y is your target data\n",
    "X_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# split data into n parts\n",
    "n_parts = len(clients)\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "for i in range(n_parts):\n",
    "    start = i * part_size\n",
    "    end = (i + 1) * part_size\n",
    "    X_part = X_train[start:end]\n",
    "    y_part = y_train[start:end]\n",
    "    dataset_parts.append((X_part, y_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 16000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dataset_parts\n",
    "x , y = a[2]\n",
    "len(x) , len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = MAML(create_model())\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 06:40:01.061593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-30 06:40:01.080695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-30 06:40:01.080919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-30 06:40:01.081812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-30 06:40:01.082009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-30 06:40:01.082171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-30 06:40:01.084296: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:371] A non-primary context 0x38efac70 for device 0 exists before initializing the StreamExecutor. The primary context is now 0x78134592cc75. We haven't verified StreamExecutor works with that.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bad StatusOr access: INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(clients)):\n\u001b[0;32m----> 3\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mmodel_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_init\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m MAML(\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      4\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m      5\u001b[0m         loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/tensorflow/python/eager/context.py:598\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(opts, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    596\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[1;32m    597\u001b[0m       opts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile_rewrite)\n\u001b[0;32m--> 598\u001b[0m   context_handle \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_NewContext(opts)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN: unknown error"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for _ in range(len(clients)):\n",
    "    models.append(model_init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "HE = Pyfhel() \n",
    "ckks_params = {\n",
    "    \"scheme\": \"CKKS\", \n",
    "    \"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "    \"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "    \"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "}\n",
    "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "HE.rotateKeyGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3, 1, 32), (32,), (3, 3, 32, 64), (64,), (3, 3, 64, 64), (64,), (576, 64), (64,), (64, 10), (10,)]\n"
     ]
    }
   ],
   "source": [
    "shapedims = [l.shape for l in models[0].get_weights()]\n",
    "print(shapedims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def generate_rsa_keys():\n",
    "    private_key = rsa.generate_private_key(public_exponent=65537, key_size=2048)\n",
    "    public_key = private_key.public_key()\n",
    "    return private_key, public_key\n",
    "\n",
    "\n",
    "def serialize_private_key(private_key):\n",
    "    pem = private_key.private_bytes(\n",
    "        encoding=serialization.Encoding.PEM,\n",
    "        format=serialization.PrivateFormat.PKCS8,\n",
    "        encryption_algorithm=serialization.NoEncryption(),\n",
    "    )\n",
    "    return pem\n",
    "\n",
    "\n",
    "def serialize_public_key(public_key):\n",
    "    pem = public_key.public_bytes(\n",
    "        encoding=serialization.Encoding.PEM,\n",
    "        format=serialization.PublicFormat.SubjectPublicKeyInfo,\n",
    "    )\n",
    "    return pem\n",
    "\n",
    "\n",
    "def encrypt_session_key(public_key, session_key):\n",
    "    encrypted_session_key = public_key.encrypt(\n",
    "        session_key,\n",
    "        padding.OAEP(\n",
    "            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
    "            algorithm=hashes.SHA256(),\n",
    "            label=None,\n",
    "        ),\n",
    "    )\n",
    "    return encrypted_session_key\n",
    "\n",
    "\n",
    "def decrypt_session_key(private_key, encrypted_session_key):\n",
    "    session_key = private_key.decrypt(\n",
    "        encrypted_session_key,\n",
    "        padding.OAEP(\n",
    "            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
    "            algorithm=hashes.SHA256(),\n",
    "            label=None,\n",
    "        ),\n",
    "    )\n",
    "    return session_key\n",
    "\n",
    "\n",
    "def generate_aes_key(length=32):\n",
    "    return os.urandom(length)\n",
    "\n",
    "\n",
    "def encrypt_message_AES(key, message):\n",
    "    serialized_obj = pickle.dumps(message)\n",
    "    nonce = os.urandom(12)\n",
    "    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce))\n",
    "    encryptor = cipher.encryptor()\n",
    "    padded_obj = serialized_obj + b\" \" * (16 - len(serialized_obj) % 16)\n",
    "    ciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "    return nonce + encryptor.tag + ciphertext\n",
    "\n",
    "\n",
    "def decrypt_message_AES(key, ciphertext):\n",
    "    nonce = ciphertext[:12]\n",
    "    tag = ciphertext[12:28]\n",
    "    enc_message = ciphertext[28:]\n",
    "    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce, tag))\n",
    "    decryptor = cipher.decryptor()\n",
    "    padded_obj = decryptor.update(enc_message) + decryptor.finalize()\n",
    "    serialized_obj = padded_obj.rstrip(b\" \")\n",
    "    message = pickle.loads(serialized_obj)\n",
    "    return message\n",
    "\n",
    "\n",
    "# Aggregator generates RSA keys\n",
    "aggregator_private_key, aggregator_public_key = generate_rsa_keys()\n",
    "\n",
    "# Serialize and store the public key for clients to use\n",
    "aggregator_public_key_pem = serialize_public_key(aggregator_public_key)\n",
    "with open(\"aggregator_public_key.pem\", \"wb\") as f:\n",
    "    f.write(aggregator_public_key_pem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_session_keys = [0]*len(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_wt(wtarray, i):\n",
    "    session_key = generate_aes_key()  # Generate new AES key for this round\n",
    "    encrypted_session_key = encrypt_session_key(aggregator_public_key, session_key)\n",
    "    encrypted_session_keys[i] = encrypted_session_key  # Save encrypted session key\n",
    "    cwt = []\n",
    "    for layer in wtarray:\n",
    "        flat_array = layer.astype(np.float64).flatten()\n",
    "        chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "        clayer = []\n",
    "        for chunk in chunks:\n",
    "            ptxt = HE.encodeFrac(chunk)\n",
    "            ctxt = HE.encryptPtxt(ptxt)\n",
    "            clayer.append(ctxt)\n",
    "        cwt.append(clayer.copy())\n",
    "    ciphertext = encrypt_message_AES(session_key, cwt)\n",
    "    return ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_keys = [0]*len(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wt(encrypted_cwts):\n",
    "    decrypted_cwts = []\n",
    "    for i, ecwt in enumerate(encrypted_cwts):\n",
    "        session_key = decrypt_session_key(\n",
    "            aggregator_private_key, encrypted_session_keys[i]\n",
    "        )\n",
    "        cwt = decrypt_message_AES(session_key, ecwt)\n",
    "        decrypted_cwts.append(cwt)\n",
    "\n",
    "    num_clients = len(decrypted_cwts)\n",
    "    num_layers = len(decrypted_cwts[0])\n",
    "\n",
    "    resmodel = []\n",
    "    for j in range(num_layers):\n",
    "        layer_chunks = []\n",
    "        num_chunks = len(decrypted_cwts[0][j])\n",
    "        for k in range(num_chunks):\n",
    "            aggregated_chunk = np.copy(decrypted_cwts[0][j][k])\n",
    "            for i in range(1, num_clients):\n",
    "                aggregated_chunk += decrypted_cwts[i][j][k]\n",
    "            aggregated_chunk /= num_clients\n",
    "            layer_chunks.append(aggregated_chunk)\n",
    "        resmodel.append(layer_chunks)\n",
    "\n",
    "    res = [resmodel.copy() for _ in range(num_clients)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_weights(res):\n",
    "    decrypted_weights = []\n",
    "    for client_weights, model in zip(res, models):\n",
    "        decrypted_client_weights = []\n",
    "        wtarray = model.get_weights()\n",
    "        for layer_weights, layer in zip(client_weights, wtarray):\n",
    "            decrypted_layer_weights = []\n",
    "            flat_array = layer.astype(np.float64).flatten()\n",
    "            chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "            for chunk, encrypted_chunk in zip(chunks, layer_weights):\n",
    "                decrypted_chunk = HE.decryptFrac(encrypted_chunk)\n",
    "                original_chunk_size = len(chunk)\n",
    "                decrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "                decrypted_layer_weights.append(decrypted_chunk)\n",
    "            decrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
    "            decrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
    "            decrypted_client_weights.append(decrypted_layer_weights)\n",
    "        decrypted_weights.append(decrypted_client_weights)\n",
    "    return decrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for _ in range(len(clients))]\n",
    "losses = [[] for _ in range(len(clients))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_batch_size = 32  # Number of tasks per meta-update\n",
    "inner_batch_size = 5  # Number of examples per task\n",
    "num_inner_updates = 5  # Number of inner loop updates per task\n",
    "num_meta_updates = 100\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    meta_updates = []\n",
    "    accuracy_over_time = []\n",
    "    for meta_update in range(num_meta_updates):\n",
    "        # Sample a meta-batch of tasks\n",
    "        meta_batch = tf.random.shuffle(tf.range(len(x_train)))[:meta_batch_size]\n",
    "\n",
    "        # Inner loop updates for each task\n",
    "        task_gradients = []\n",
    "        for task in meta_batch:\n",
    "            task_data = (\n",
    "                x_train[task : task + inner_batch_size],\n",
    "                y_train[task : task + inner_batch_size],\n",
    "            )\n",
    "\n",
    "            with tf.GradientTape() as outer_tape:\n",
    "                with tf.GradientTape() as inner_tape:\n",
    "                    # Forward pass on the task-specific data\n",
    "                    predictions = model(task_data[0])\n",
    "                    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                        task_data[1], predictions\n",
    "                    )\n",
    "\n",
    "                # Compute gradients for inner loop update\n",
    "                inner_gradients = inner_tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                # Apply inner loop update to the model's variables\n",
    "                inner_model = MAML(create_model())\n",
    "                inner_model.set_weights(model.get_weights())\n",
    "                optimizer.apply_gradients(\n",
    "                    zip(inner_gradients, inner_model.trainable_variables)\n",
    "                )\n",
    "\n",
    "                # Forward pass with the updated model\n",
    "                updated_predictions = inner_model(task_data[0])\n",
    "                updated_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                    task_data[1], updated_predictions\n",
    "                )\n",
    "\n",
    "            # Compute gradients for outer loop update\n",
    "            outer_gradients = outer_tape.gradient(\n",
    "                updated_loss, model.trainable_variables\n",
    "            )\n",
    "            task_gradients.append(outer_gradients)\n",
    "\n",
    "        # Filter out None gradients\n",
    "        filtered_task_gradients = [\n",
    "            [grad for grad in task_grad if grad is not None]\n",
    "            for task_grad in task_gradients\n",
    "        ]\n",
    "\n",
    "        # Compute average gradients across tasks\n",
    "        avg_gradients = [\n",
    "            tf.reduce_mean(grad_list, axis=0)\n",
    "            for grad_list in zip(*filtered_task_gradients)\n",
    "        ]\n",
    "\n",
    "        # Apply outer loop update to the model's variables\n",
    "        optimizer.apply_gradients(zip(avg_gradients, model.trainable_variables))\n",
    "\n",
    "        # Evaluate on the meta-test set\n",
    "        _, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "        # Store the meta-update step and accuracy\n",
    "        meta_updates.append(meta_update + 1)\n",
    "        accuracy_over_time.append(accuracy)\n",
    "\n",
    "    avg_accuracy = sum(accuracy_over_time) / len(accuracy_over_time)\n",
    "    return model, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2024-05-29 18:03:57.696670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-05-29 18:03:57.754856: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:57.755238: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:57.755268: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:57.755496: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:57.755541: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:322] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-05-29 18:03:57.816650: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:521] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  /home/voy/.conda/envs/he39/lib/python3.9/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/voy/.conda/envs/he39/lib/python3.9/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-05-29 18:03:57.822642: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:57.823010: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:57.823030: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:57.823238: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:57.823271: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.000116: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000159: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.000429: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000448: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000467: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.000502: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.000593: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000613: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.000706: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000739: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.000848: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000883: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.000902: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.000922: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.001114: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.001148: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.001482: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.001504: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.001724: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.001756: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.003540: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.003562: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.003777: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.003806: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.005135: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.005158: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.005338: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.005357: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.005371: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.005390: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.005559: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.005587: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.192028: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.192062: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.192405: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.192426: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.192448: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.192474: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.192688: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.192729: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.193031: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.193052: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.193275: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.193295: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.193416: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.193451: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.193537: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.193559: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.193576: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.193596: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.193775: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.193809: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.195385: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.195414: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.195751: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.195792: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.196047: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.196070: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.196357: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.196398: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.197333: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.197357: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-29 18:03:58.197573: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-29 18:03:58.197601: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-29 18:03:58.339825: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:559] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2024-05-29 18:03:58.340876: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: JIT compilation failed.\n",
      "  0%|          | 0/20 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m wt,model , dataset , i , \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(wts, models, dataset_parts , \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(clients))):\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_weights(wt)\n\u001b[0;32m----> 8\u001b[0m     model, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# history = model.fit(dataset[0], dataset[1], epochs=1,  verbose=1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(history.history[\"accuracy\"][0], history.history[\"loss\"][0])\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# accuracies[i ].append(history.history[\"accuracy\"][0])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# losses[i].append(history.history[\"loss\"][0])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m inner_tape:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Forward pass on the task-specific data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(task_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compute gradients for inner loop update\u001b[39;00m\n\u001b[1;32m     25\u001b[0m inner_gradients \u001b[38;5;241m=\u001b[39m inner_tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/keras/src/losses.py:2454\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[1;32m   2404\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.metrics.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2409\u001b[0m     y_true, y_pred, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ignore_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m ):\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sparse categorical crossentropy loss.\u001b[39;00m\n\u001b[1;32m   2412\u001b[0m \n\u001b[1;32m   2413\u001b[0m \u001b[38;5;124;03m    Standalone usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[38;5;124;03m        Sparse categorical crossentropy loss value.\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/keras/src/backend.py:5735\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   5733\u001b[0m     epsilon_ \u001b[38;5;241m=\u001b[39m _constant_to_tensor(epsilon(), output\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype)\n\u001b[1;32m   5734\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(output, epsilon_, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon_)\n\u001b[0;32m-> 5735\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5737\u001b[0m \u001b[38;5;66;03m# Permute output so that the last axis contains the logits/probabilities.\u001b[39;00m\n\u001b[1;32m   5738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log] name: "
     ]
    }
   ],
   "source": [
    "cwts = [encrypt_wt(model.get_weights() , i) for i , model in enumerate(models)]\n",
    "for e in tqdm(range(epochs)):\n",
    "    cwts = aggregate_wt(cwts)\n",
    "    wts = decrypt_weights(cwts)\n",
    "    cwts = []\n",
    "    for wt,model , dataset , i , in zip(wts, models, dataset_parts , range(len(clients))):\n",
    "        model.set_weights(wt)\n",
    "        model, accuracy = train_model(model, dataset[0], dataset[1])\n",
    "        # history = model.fit(dataset[0], dataset[1], epochs=1,  verbose=1)\n",
    "        # print(history.history[\"accuracy\"][0], history.history[\"loss\"][0])\n",
    "        # accuracies[i ].append(history.history[\"accuracy\"][0])\n",
    "        # losses[i].append(history.history[\"loss\"][0])\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"accuracies\" , accuracy)\n",
    "        wtarray = model.get_weights()\n",
    "        cwts.append(encrypt_wt(wtarray , i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, client in enumerate(clients):\n",
    "    plt.plot(\n",
    "        epochs_range,\n",
    "        accuracies[i],\n",
    "        label=f\"Client {client}\" if client != 0 else \"aggregate\",\n",
    "    )\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy for Each Client\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, client in enumerate(clients):\n",
    "    plt.plot(epochs_range, losses[i], label=f\"Client {client}\" if client != 0 else \"aggregate\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss for Each Client\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# for i, client in enumerate(clients):\n",
    "plt.plot(epochs_range, accuracies[0], label=f\"Client {client}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy for Each Client\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = models[0]\n",
    "# Assuming you have a trained model named 'model'\n",
    "# and input data 'X_test' and corresponding labels 'y_test'\n",
    "\n",
    "# Select a sample image from the test set\n",
    "# Select a sample image from the test set\n",
    "sample_image = X_test[1]  # Adjust the index as needed\n",
    "sample_label = y_test[1]\n",
    "\n",
    "# Preprocess the sample image\n",
    "sample_image = sample_image[np.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "# Initialize the model\n",
    "# model = model_init()\n",
    "\n",
    "# Create a model that outputs the activations of the first dense layer\n",
    "layer_name = \"dense_40\"  # Name of the first dense layer in your model\n",
    "activation_model = Model(\n",
    "    inputs=model.inputs, outputs=model.get_layer(layer_name).output\n",
    ")\n",
    "\n",
    "# Get the activations of the first dense layer\n",
    "activations = activation_model.predict(sample_image)\n",
    "\n",
    "# Plot the sample image and the activation map\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(sample_image[0], cmap=\"gray\")  # Assuming grayscale image\n",
    "ax1.set_title(\"Sample Image\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "ax2.imshow(activations[0].reshape(2, 4), cmap=\"viridis\", interpolation=\"nearest\")\n",
    "ax2.set_title(\"Activation Map\")\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

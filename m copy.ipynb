{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives import padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import os\n",
    "import pickle\n",
    "import os\n",
    "import pickle\n",
    "import pickle\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [0 , 1, 2]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train_all = x_train_all.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# # Define the MAML model\n",
    "# class MAML(tf.keras.Model):\n",
    "#     def __init__(self, model):\n",
    "#         super(MAML, self).__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def train_step(self, data):\n",
    "#         x, y = data\n",
    "#         x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "#         y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = self.model(x)\n",
    "#             loss = self.compiled_loss(y, y_pred)\n",
    "#         gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "#         self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "#     def test_step(self, data):\n",
    "#         x, y = data\n",
    "#         x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "#         y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "#         y_pred = self.model(x)\n",
    "#         self.compiled_loss(y, y_pred)\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(MAML, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "        return self.model(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"model\": self.model.get_config()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        model = tf.keras.models.Model.from_config(config[\"model\"])\n",
    "        return cls(model)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "        y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "        y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "        y_pred = self.model(x)\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "# Define the meta-learning parameters\n",
    "num_meta_updates = 10\n",
    "num_inner_updates = 5\n",
    "meta_batch_size = 32\n",
    "inner_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume X is your feature data and y is your target data\n",
    "X_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# split data into n parts\n",
    "n_parts = len(clients)\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "for i in range(n_parts):\n",
    "    start = i * part_size\n",
    "    end = (i + 1) * part_size\n",
    "    X_part = X_train[start:end]\n",
    "    y_part = y_train[start:end]\n",
    "    dataset_parts.append((X_part, y_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 16000)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dataset_parts\n",
    "x , y = a[2]\n",
    "len(x) , len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = MAML(create_model())\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for _ in range(len(clients)):\n",
    "    models.append(model_init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "HE = Pyfhel() \n",
    "ckks_params = {\n",
    "    \"scheme\": \"CKKS\", \n",
    "    \"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "    \"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "    \"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "}\n",
    "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "HE.rotateKeyGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3, 1, 32), (32,), (3, 3, 32, 64), (64,), (3, 3, 64, 64), (64,), (576, 64), (64,), (64, 10), (10,)]\n"
     ]
    }
   ],
   "source": [
    "shapedims = [l.shape for l in models[0].get_weights()]\n",
    "print(shapedims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def generate_rsa_keys():\n",
    "    private_key = rsa.generate_private_key(public_exponent=65537, key_size=2048)\n",
    "    public_key = private_key.public_key()\n",
    "    return private_key, public_key\n",
    "\n",
    "\n",
    "def serialize_private_key(private_key):\n",
    "    pem = private_key.private_bytes(\n",
    "        encoding=serialization.Encoding.PEM,\n",
    "        format=serialization.PrivateFormat.PKCS8,\n",
    "        encryption_algorithm=serialization.NoEncryption(),\n",
    "    )\n",
    "    return pem\n",
    "\n",
    "\n",
    "def serialize_public_key(public_key):\n",
    "    pem = public_key.public_bytes(\n",
    "        encoding=serialization.Encoding.PEM,\n",
    "        format=serialization.PublicFormat.SubjectPublicKeyInfo,\n",
    "    )\n",
    "    return pem\n",
    "\n",
    "\n",
    "def encrypt_session_key(public_key, session_key):\n",
    "    encrypted_session_key = public_key.encrypt(\n",
    "        session_key,\n",
    "        padding.OAEP(\n",
    "            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
    "            algorithm=hashes.SHA256(),\n",
    "            label=None,\n",
    "        ),\n",
    "    )\n",
    "    return encrypted_session_key\n",
    "\n",
    "\n",
    "def decrypt_session_key(private_key, encrypted_session_key):\n",
    "    session_key = private_key.decrypt(\n",
    "        encrypted_session_key,\n",
    "        padding.OAEP(\n",
    "            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
    "            algorithm=hashes.SHA256(),\n",
    "            label=None,\n",
    "        ),\n",
    "    )\n",
    "    return session_key\n",
    "\n",
    "\n",
    "def generate_aes_key(length=32):\n",
    "    return os.urandom(length)\n",
    "\n",
    "\n",
    "def encrypt_message_AES(key, message):\n",
    "    serialized_obj = pickle.dumps(message)\n",
    "    nonce = os.urandom(12)\n",
    "    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce))\n",
    "    encryptor = cipher.encryptor()\n",
    "    padded_obj = serialized_obj + b\" \" * (16 - len(serialized_obj) % 16)\n",
    "    ciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "    return nonce + encryptor.tag + ciphertext\n",
    "\n",
    "\n",
    "def decrypt_message_AES(key, ciphertext):\n",
    "    nonce = ciphertext[:12]\n",
    "    tag = ciphertext[12:28]\n",
    "    enc_message = ciphertext[28:]\n",
    "    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce, tag))\n",
    "    decryptor = cipher.decryptor()\n",
    "    padded_obj = decryptor.update(enc_message) + decryptor.finalize()\n",
    "    serialized_obj = padded_obj.rstrip(b\" \")\n",
    "    message = pickle.loads(serialized_obj)\n",
    "    return message\n",
    "\n",
    "\n",
    "# Aggregator generates RSA keys\n",
    "aggregator_private_key, aggregator_public_key = generate_rsa_keys()\n",
    "\n",
    "# Serialize and store the public key for clients to use\n",
    "aggregator_public_key_pem = serialize_public_key(aggregator_public_key)\n",
    "with open(\"aggregator_public_key.pem\", \"wb\") as f:\n",
    "    f.write(aggregator_public_key_pem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_session_keys = [0]*len(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_wt(wtarray, i):\n",
    "    session_key = generate_aes_key()  # Generate new AES key for this round\n",
    "    encrypted_session_key = encrypt_session_key(aggregator_public_key, session_key)\n",
    "    encrypted_session_keys[i] = encrypted_session_key  # Save encrypted session key\n",
    "    cwt = []\n",
    "    for layer in wtarray:\n",
    "        flat_array = layer.astype(np.float64).flatten()\n",
    "        chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "        clayer = []\n",
    "        for chunk in chunks:\n",
    "            ptxt = HE.encodeFrac(chunk)\n",
    "            ctxt = HE.encryptPtxt(ptxt)\n",
    "            clayer.append(ctxt)\n",
    "        cwt.append(clayer.copy())\n",
    "    ciphertext = encrypt_message_AES(session_key, cwt)\n",
    "    return ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_keys = [0]*len(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wt(encrypted_cwts):\n",
    "    decrypted_cwts = []\n",
    "    for i, ecwt in enumerate(encrypted_cwts):\n",
    "        session_key = decrypt_session_key(\n",
    "            aggregator_private_key, encrypted_session_keys[i]\n",
    "        )\n",
    "        cwt = decrypt_message_AES(session_key, ecwt)\n",
    "        decrypted_cwts.append(cwt)\n",
    "\n",
    "    num_clients = len(decrypted_cwts)\n",
    "    num_layers = len(decrypted_cwts[0])\n",
    "\n",
    "    resmodel = []\n",
    "    for j in range(num_layers):\n",
    "        layer_chunks = []\n",
    "        num_chunks = len(decrypted_cwts[0][j])\n",
    "        for k in range(num_chunks):\n",
    "            aggregated_chunk = np.copy(decrypted_cwts[0][j][k])\n",
    "            for i in range(1, num_clients):\n",
    "                aggregated_chunk += decrypted_cwts[i][j][k]\n",
    "            aggregated_chunk /= num_clients\n",
    "            layer_chunks.append(aggregated_chunk)\n",
    "        resmodel.append(layer_chunks)\n",
    "\n",
    "    res = [resmodel.copy() for _ in range(num_clients)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_weights(res):\n",
    "    decrypted_weights = []\n",
    "    for client_weights, model in zip(res, models):\n",
    "        decrypted_client_weights = []\n",
    "        wtarray = model.get_weights()\n",
    "        for layer_weights, layer in zip(client_weights, wtarray):\n",
    "            decrypted_layer_weights = []\n",
    "            for encrypted_chunk in layer_weights:\n",
    "                decrypted_chunk = []\n",
    "                for ctxt in encrypted_chunk:\n",
    "                    decrypted_chunk.extend(HE.decryptFrac(ctxt))\n",
    "                decrypted_layer_weights.append(np.array(decrypted_chunk))\n",
    "            decrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
    "            decrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
    "            decrypted_client_weights.append(decrypted_layer_weights)\n",
    "        decrypted_weights.append(decrypted_client_weights)\n",
    "    return decrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for _ in range(len(clients))]\n",
    "losses = [[] for _ in range(len(clients))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_session_keys = [None] * len(clients)\n",
    "shared_keys = [None] * len(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_batch_size = 32  # Number of tasks per meta-update\n",
    "inner_batch_size = 1  # Number of examples per task\n",
    "num_inner_updates = 1\n",
    "# Number of inner loop updates per task\n",
    "num_meta_updates = 10\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    meta_updates = []\n",
    "    accuracy_over_time = []\n",
    "    for meta_update in range(num_meta_updates):\n",
    "        meta_batch = tf.random.shuffle(tf.range(len(x_train)))[:meta_batch_size]\n",
    "        task_gradients = []\n",
    "        for task in meta_batch:\n",
    "            task_data = (\n",
    "                x_train[task: task + inner_batch_size],\n",
    "                y_train[task: task + inner_batch_size],\n",
    "            )\n",
    "            with tf.GradientTape() as outer_tape:\n",
    "                with tf.GradientTape() as inner_tape:\n",
    "                    predictions = model(task_data[0])\n",
    "                    loss = tf.keras.losses.sparse_categorical_crossentropy(task_data[1], predictions)\n",
    "                inner_gradients = inner_tape.gradient(loss, model.trainable_variables)\n",
    "                inner_model = MAML(create_model())\n",
    "                inner_model.set_weights(model.get_weights())\n",
    "                optimizer.apply_gradients(zip(inner_gradients, inner_model.trainable_variables))\n",
    "                updated_predictions = inner_model(task_data[0])\n",
    "                updated_loss = tf.keras.losses.sparse_categorical_crossentropy(task_data[1], updated_predictions)\n",
    "            outer_gradients = outer_tape.gradient(updated_loss, model.trainable_variables)\n",
    "            task_gradients.append(outer_gradients)\n",
    "        filtered_task_gradients = [[grad for grad in task_grad if grad is not None] for task_grad in task_gradients]\n",
    "        avg_gradients = [tf.reduce_mean(grad_list, axis=0) for grad_list in zip(*filtered_task_gradients)]\n",
    "        optimizer.apply_gradients(zip(avg_gradients, model.trainable_variables))\n",
    "        _, accuracy = model.evaluate(x_test, y_test)\n",
    "        meta_updates.append(meta_update + 1)\n",
    "        accuracy_over_time.append(accuracy)\n",
    "    avg_accuracy = sum(accuracy_over_time) / len(accuracy_over_time)\n",
    "    return model, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                     | 0/20 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m cwts \u001b[38;5;241m=\u001b[39m [encrypt_wt(model\u001b[38;5;241m.\u001b[39mget_weights(), i) \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models)]\n\u001b[1;32m      3\u001b[0m cwts \u001b[38;5;241m=\u001b[39m aggregate_wt(cwts)\n\u001b[0;32m----> 4\u001b[0m wts \u001b[38;5;241m=\u001b[39m \u001b[43mdecrypt_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcwts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (wt, model, dataset) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(wts, models, dataset_parts)):\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_weights(wt)\n",
      "Cell \u001b[0;32mIn[144], line 10\u001b[0m, in \u001b[0;36mdecrypt_weights\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encrypted_chunk \u001b[38;5;129;01min\u001b[39;00m layer_weights:\n\u001b[1;32m      9\u001b[0m     decrypted_chunk \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ctxt \u001b[38;5;129;01min\u001b[39;00m encrypted_chunk:\n\u001b[1;32m     11\u001b[0m         decrypted_chunk\u001b[38;5;241m.\u001b[39mextend(HE\u001b[38;5;241m.\u001b[39mdecryptFrac(ctxt))\n\u001b[1;32m     12\u001b[0m     decrypted_layer_weights\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(decrypted_chunk))\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(epochs)):\n",
    "    cwts = [encrypt_wt(model.get_weights(), i) for i, model in enumerate(models)]\n",
    "    cwts = aggregate_wt(cwts)\n",
    "    wts = decrypt_weights(cwts)\n",
    "    for i, (wt, model, dataset) in enumerate(zip(wts, models, dataset_parts)):\n",
    "        model.set_weights(wt)\n",
    "        model, accuracy = train_model(model, dataset[0], dataset[1])\n",
    "        accuracies[i].append(accuracy)\n",
    "        print(f\"Client {i} accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, client in enumerate(clients):\n",
    "    plt.plot(\n",
    "        epochs_range,\n",
    "        accuracies[i],\n",
    "        label=f\"Client {client}\" if client != 0 else \"aggregate\",\n",
    "    )\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy for Each Client\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, client in enumerate(clients):\n",
    "    plt.plot(epochs_range, losses[i], label=f\"Client {client}\" if client != 0 else \"aggregate\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss for Each Client\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# for i, client in enumerate(clients):\n",
    "plt.plot(epochs_range, accuracies[0], label=f\"Client {client}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy for Each Client\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = models[0]\n",
    "# Assuming you have a trained model named 'model'\n",
    "# and input data 'X_test' and corresponding labels 'y_test'\n",
    "\n",
    "# Select a sample image from the test set\n",
    "# Select a sample image from the test set\n",
    "sample_image = X_test[1]  # Adjust the index as needed\n",
    "sample_label = y_test[1]\n",
    "\n",
    "# Preprocess the sample image\n",
    "sample_image = sample_image[np.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "# Initialize the model\n",
    "# model = model_init()\n",
    "\n",
    "# Create a model that outputs the activations of the first dense layer\n",
    "layer_name = \"dense_40\"  # Name of the first dense layer in your model\n",
    "activation_model = Model(\n",
    "    inputs=model.inputs, outputs=model.get_layer(layer_name).output\n",
    ")\n",
    "\n",
    "# Get the activations of the first dense layer\n",
    "activations = activation_model.predict(sample_image)\n",
    "\n",
    "# Plot the sample image and the activation map\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(sample_image[0], cmap=\"gray\")  # Assuming grayscale image\n",
    "ax1.set_title(\"Sample Image\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "ax2.imshow(activations[0].reshape(2, 4), cmap=\"viridis\", interpolation=\"nearest\")\n",
    "ax2.set_title(\"Activation Map\")\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

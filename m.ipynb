{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 06:25:41.881276: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-26 06:25:41.912826: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 06:25:41.912858: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 06:25:41.912884: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-26 06:25:41.919386: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-26 06:25:41.920169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 06:25:42.700077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [1, 2, 3]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train_all = x_train_all.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume X is your feature data and y is your target data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# split data into n parts\n",
    "n_parts = len(clients)\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "for i in range(n_parts):\n",
    "    start = i * part_size\n",
    "    end = (i + 1) * part_size\n",
    "    X_part = X_train[start:end]\n",
    "    y_part = y_train[start:end]\n",
    "    dataset_parts.append((X_part, y_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 06:25:44.396370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-26 06:25:44.418606: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for _ in range(len(clients)):\n",
    "    models.append(model_init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "HE = Pyfhel()  # Creating empty Pyfhel object\n",
    "ckks_params = {\n",
    "    \"scheme\": \"CKKS\",  # can also be 'ckks'\n",
    "    \"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "    #  encoded in a single ciphertext.\n",
    "    #  Typ. 2^D for D in [10, 15]\n",
    "    \"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "    #  conversion: x_fix = round(x_float * scale)\n",
    "    #  You can use this as default scale or use a different\n",
    "    #  scale on each operation (set in HE.encryptFrac)\n",
    "    \"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "    # Intermediate values should be  close to log2(scale)\n",
    "    # for each operation, to have small rounding errors.\n",
    "}\n",
    "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "HE.rotateKeyGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(784, 128), (128,), (128, 10), (10,)]\n"
     ]
    }
   ],
   "source": [
    "shapedims = [l.shape for l in models[0].get_weights()]\n",
    "print(shapedims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_wt(wtarray):\n",
    "    cwt = []\n",
    "    for layer in wtarray:\n",
    "        flat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "        chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "        clayer = []\n",
    "        for chunk in chunks:\n",
    "            ptxt = HE.encodeFrac(chunk)\n",
    "            ctxt = HE.encryptPtxt(ptxt)\n",
    "            clayer.append(ctxt)\n",
    "        cwt.append(clayer.copy())\n",
    "    return cwt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wt(cwts):\n",
    "    resmodel = []\n",
    "    for j in range(len(cwts[0])):  # for layers\n",
    "        layer = []\n",
    "        for k in range(len(cwts[0][j])):  # for chunks\n",
    "            tmp = cwts[0][j][k].copy()\n",
    "            for i in range(1, len(cwts)):  # for clients\n",
    "                tmp = tmp + cwts[i][j][k]\n",
    "            tmp = tmp / len(cwts)\n",
    "            layer.append(tmp)\n",
    "        resmodel.append(layer)\n",
    "\n",
    "    res = [resmodel.copy() for _ in range(len(clients))]\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_weights(res):\n",
    "    decrypted_weights = []\n",
    "    for client_weights, model in zip(res, models):\n",
    "        decrypted_client_weights = []\n",
    "        wtarray = model.get_weights()\n",
    "        for layer_weights, layer in zip(client_weights, wtarray):\n",
    "            decrypted_layer_weights = []\n",
    "            flat_array = layer.astype(np.float64).flatten()\n",
    "            chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "            for chunk, encrypted_chunk in zip(chunks, layer_weights):\n",
    "                decrypted_chunk = HE.decryptFrac(encrypted_chunk)\n",
    "                original_chunk_size = len(chunk)\n",
    "                decrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "                decrypted_layer_weights.append(decrypted_chunk)\n",
    "            decrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
    "            decrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
    "            decrypted_client_weights.append(decrypted_layer_weights)\n",
    "        decrypted_weights.append(decrypted_client_weights)\n",
    "    return decrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0212 - accuracy: 0.9948\n",
      "500/500 [==============================] - 0s 979us/step - loss: 0.0205 - accuracy: 0.9954\n",
      "500/500 [==============================] - 0s 990us/step - loss: 0.0213 - accuracy: 0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 1/20 [00:05<01:43,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0307 - accuracy: 0.9906\n",
      "500/500 [==============================] - 0s 979us/step - loss: 0.0333 - accuracy: 0.9908\n",
      "500/500 [==============================] - 0s 975us/step - loss: 0.0301 - accuracy: 0.9902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 2/20 [00:11<01:39,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9923\n",
      "500/500 [==============================] - 0s 994us/step - loss: 0.0297 - accuracy: 0.9915\n",
      "500/500 [==============================] - 0s 982us/step - loss: 0.0230 - accuracy: 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 3/20 [00:16<01:34,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 962us/step - loss: 0.0255 - accuracy: 0.9921\n",
      "500/500 [==============================] - 0s 996us/step - loss: 0.0250 - accuracy: 0.9923\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0222 - accuracy: 0.9939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 4/20 [00:22<01:28,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 989us/step - loss: 0.0210 - accuracy: 0.9941\n",
      "500/500 [==============================] - 0s 992us/step - loss: 0.0227 - accuracy: 0.9931\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0189 - accuracy: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:28<01:25,  5.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cwts \u001b[38;5;241m=\u001b[39m [encrypt_wt(model\u001b[38;5;241m.\u001b[39mget_weights()) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m----> 3\u001b[0m     wts \u001b[38;5;241m=\u001b[39m \u001b[43mdecrypt_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcwts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     cwts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m wt,model , dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(wts, models, dataset_parts):\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mdecrypt_weights\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m      9\u001b[0m chunks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(flat_array, (\u001b[38;5;28mlen\u001b[39m(flat_array) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, encrypted_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, layer_weights):\n\u001b[0;32m---> 11\u001b[0m     decrypted_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mHE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecryptFrac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencrypted_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     original_chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m     13\u001b[0m     decrypted_chunk \u001b[38;5;241m=\u001b[39m decrypted_chunk[:original_chunk_size]\n",
      "File \u001b[0;32mPyfhel/Pyfhel.pyx:546\u001b[0m, in \u001b[0;36mPyfhel.Pyfhel.Pyfhel.decryptFrac\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mPyfhel/Pyfhel.pyx:567\u001b[0m, in \u001b[0;36mPyfhel.Pyfhel.Pyfhel.decryptFrac\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cwts = [encrypt_wt(model.get_weights()) for model in models]\n",
    "for e in tqdm(range(epochs)):\n",
    "    wts = decrypt_weights(cwts)\n",
    "    cwts = []\n",
    "    for wt,model , dataset in zip(wts, models, dataset_parts):\n",
    "        model.set_weights(wt)\n",
    "        model.fit(dataset[0], dataset[1], epochs=1,  verbose=1)\n",
    "        wtarray = model.get_weights()\n",
    "        cwts.append(encrypt_wt(wtarray))\n",
    "\n",
    "    cwts = aggregate_wt(cwts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

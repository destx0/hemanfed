{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7RCTjRCj-f8h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "from cryptography.hazmat.primitives import hashes, serialization\n",
        "from cryptography.hazmat.primitives.asymmetric import dh\n",
        "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
        "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
        "import pickle\n",
        "import sys\n",
        "import pickle\n",
        "import nacl.utils\n",
        "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
        "from cryptography.hazmat.primitives import padding\n",
        "from cryptography.hazmat.backends import default_backend\n",
        "import nacl.utils\n",
        "from nacl.public import PrivateKey, SealedBox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV3GFnJmAlOJ",
        "outputId": "4fe81dc6-658d-4be6-c734-271d1e8a4588"
      },
      "outputs": [],
      "source": [
        "# !pip install Pyfhel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DfC7h9QW-f8j"
      },
      "outputs": [],
      "source": [
        "clients = [0 , 1, 2]\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "client_ses_key = [None for _ in range(len(clients))]\n",
        "client_enc_ses_key = [None for _ in range(len(clients))]\n",
        "agg_ses_key = [None for _ in range(len(clients))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPouyDo_-f8j",
        "outputId": "c9e08777-12f5-432c-90a7-7278baa5c5c1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train_all = x_train_all.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RWdOVhTG-f8k"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Conv2D(\n",
        "                32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
        "            ),\n",
        "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(10),\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# # Define the MAML model\n",
        "# class MAML(tf.keras.Model):\n",
        "#     def __init__(self, model):\n",
        "#         super(MAML, self).__init__()\n",
        "#         self.model = model\n",
        "\n",
        "#     def train_step(self, data):\n",
        "#         x, y = data\n",
        "#         x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
        "#         y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             y_pred = self.model(x)\n",
        "#             loss = self.compiled_loss(y, y_pred)\n",
        "#         gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "#         self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "#         self.compiled_metrics.update_state(y, y_pred)\n",
        "#         return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "#     def test_step(self, data):\n",
        "#         x, y = data\n",
        "#         x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
        "#         y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
        "#         y_pred = self.model(x)\n",
        "#         self.compiled_loss(y, y_pred)\n",
        "#         self.compiled_metrics.update_state(y, y_pred)\n",
        "#         return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "class MAML(tf.keras.Model):\n",
        "    def __init__(self, model):\n",
        "        super(MAML, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
        "        return self.model(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"model\": self.model.get_config()}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        model = tf.keras.models.Model.from_config(config[\"model\"])\n",
        "        return cls(model)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
        "        y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self.model(x)\n",
        "            loss = self.compiled_loss(y, y_pred)\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        x = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
        "        y = tf.reshape(y, (-1,))  # Reshape the target labels\n",
        "        y_pred = self.model(x)\n",
        "        self.compiled_loss(y, y_pred)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "# Define the meta-learning parameters\n",
        "num_meta_updates = 10\n",
        "num_inner_updates = 5\n",
        "meta_batch_size = 32\n",
        "inner_batch_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "f3zxPc-I-f8k"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# assume X is your feature data and y is your target data\n",
        "X_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_train_all, y_train_all, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# split data into n parts\n",
        "n_parts = len(clients)\n",
        "part_size = len(X_train) // n_parts\n",
        "dataset_parts = []\n",
        "for i in range(n_parts):\n",
        "    start = i * part_size\n",
        "    end = (i + 1) * part_size\n",
        "    X_part = X_train[start:end]\n",
        "    y_part = y_train[start:end]\n",
        "    dataset_parts.append((X_part, y_part))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELp6c-31-f8k",
        "outputId": "b62fde4c-0c91-4a65-f38f-2bfcfbd0b61d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16000, 16000)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = dataset_parts\n",
        "x , y = a[2]\n",
        "len(x) , len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nDP42C4y-f8l"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    model = MAML(create_model())\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "E5E3ZJzJ-f8l"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "for _ in range(len(clients)):\n",
        "    models.append(model_init())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nzqBCk2N-f8l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from Pyfhel import Pyfhel\n",
        "\n",
        "HE = Pyfhel()\n",
        "ckks_params = {\n",
        "    \"scheme\": \"CKKS\",\n",
        "    \"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
        "    \"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
        "    \"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
        "}\n",
        "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
        "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
        "HE.rotateKeyGen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mexn0M05-f8l",
        "outputId": "a9980486-6eb2-4586-86dc-3e6b53075249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(3, 3, 1, 32), (32,), (3, 3, 32, 64), (64,), (3, 3, 64, 64), (64,), (576, 64), (64,), (64, 10), (10,)]\n"
          ]
        }
      ],
      "source": [
        "shapedims = [l.shape for l in models[0].get_weights()]\n",
        "print(shapedims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lJTt0tMF-f8m"
      },
      "outputs": [],
      "source": [
        "def generate_diffie_hellman_parameters():\n",
        "    parameters = dh.generate_parameters(generator=2, key_size=512)\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def generate_diffie_hellman_keys(parameters):\n",
        "    private_key = parameters.generate_private_key()\n",
        "    public_key = private_key.public_key()\n",
        "    return private_key, public_key\n",
        "\n",
        "\n",
        "def derive_key(private_key, peer_public_key):\n",
        "    shared_key = private_key.exchange(peer_public_key)\n",
        "    derived_key = HKDF(\n",
        "        algorithm=hashes.SHA256(),\n",
        "        length=32,\n",
        "        salt=None,\n",
        "        info=b\"handshake data\",\n",
        "    ).derive(shared_key)\n",
        "    return derived_key\n",
        "\n",
        "\n",
        "def encrypt_message_AES(key, message):\n",
        "    # Serialize the message\n",
        "    serialized_obj = pickle.dumps(message)\n",
        "\n",
        "    # Generate a random IV\n",
        "    iv = nacl.utils.random(16)\n",
        "\n",
        "    # Pad the serialized object to a multiple of the block size\n",
        "    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n",
        "    padded_obj = padder.update(serialized_obj) + padder.finalize()\n",
        "\n",
        "    # Encrypt using AES in CBC mode\n",
        "    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
        "    encryptor = cipher.encryptor()\n",
        "    ciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
        "\n",
        "    # Prepend the IV to the ciphertext\n",
        "    return iv + ciphertext\n",
        "\n",
        "\n",
        "def decrypt_message_AES(key, ciphertext):\n",
        "    # Extract the IV from the beginning of the ciphertext\n",
        "    iv = ciphertext[:16]\n",
        "    ciphertext = ciphertext[16:]\n",
        "\n",
        "    # Decrypt using AES in CBC mode\n",
        "    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
        "    decryptor = cipher.decryptor()\n",
        "    padded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
        "\n",
        "    # Unpad the decrypted object\n",
        "    unpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()\n",
        "    serialized_obj = unpadder.update(padded_obj) + unpadder.finalize()\n",
        "\n",
        "    # Deserialize the object\n",
        "    obj = pickle.loads(serialized_obj)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def setup_AES():\n",
        "    num_clients = len(clients)\n",
        "    parameters = generate_diffie_hellman_parameters()\n",
        "    server_private_key, server_public_key = generate_diffie_hellman_keys(parameters)\n",
        "    client_keys = [generate_diffie_hellman_keys(parameters) for _ in range(num_clients)]\n",
        "    shared_keys = [\n",
        "        derive_key(server_private_key, client_public_key)\n",
        "        for _, client_public_key in client_keys\n",
        "    ]\n",
        "    client_shared_keys = [\n",
        "        derive_key(client_private_key, server_public_key)\n",
        "        for client_private_key, _ in client_keys\n",
        "    ]\n",
        "    session_key = nacl.utils.random(32)\n",
        "\n",
        "    return client_keys, shared_keys , client_shared_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "bj2slk6e-f8m"
      },
      "outputs": [],
      "source": [
        "client_keys, shared_keys, client_shared_keys = setup_AES()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHgAG3Jn-f8m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wHsUjBb--f8m"
      },
      "outputs": [],
      "source": [
        "def encrypt_wt(wtarray , i):\n",
        "    cwt = []\n",
        "    for layer in wtarray:\n",
        "        flat_array = layer.astype(np.float64).flatten()\n",
        "\n",
        "        chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
        "        clayer = []\n",
        "        for chunk in chunks:\n",
        "            ptxt = HE.encodeFrac(chunk)\n",
        "            ctxt = HE.encryptPtxt(ptxt)\n",
        "            clayer.append(ctxt)\n",
        "        cwt.append(clayer.copy())\n",
        "    ciphertext = encrypt_message_AES(client_shared_keys[i], cwt)\n",
        "    return ciphertext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "t6Km52SF-f8m"
      },
      "outputs": [],
      "source": [
        "def aggregate_wt(encypted_cwts):\n",
        "    cwts = []\n",
        "    for i , ecwt in enumerate(encypted_cwts):\n",
        "        cwts.append(decrypt_message_AES(shared_keys[i], ecwt))\n",
        "    resmodel = []\n",
        "    for j in range(len(cwts[0])):  # for layers\n",
        "        layer = []\n",
        "        for k in range(len(cwts[0][j])):  # for chunks\n",
        "            tmp = cwts[0][j][k].copy()\n",
        "            for i in range(1, len(cwts)):  # for clients\n",
        "                tmp = tmp + cwts[i][j][k]\n",
        "            tmp = tmp / len(cwts)\n",
        "            layer.append(tmp)\n",
        "        resmodel.append(layer)\n",
        "\n",
        "    res = [resmodel.copy() for _ in range(len(clients))]\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IpyPJvSo-f8m"
      },
      "outputs": [],
      "source": [
        "def decrypt_weights(res):\n",
        "    decrypted_weights = []\n",
        "    for client_weights, model in zip(res, models):\n",
        "        decrypted_client_weights = []\n",
        "        wtarray = model.get_weights()\n",
        "        for layer_weights, layer in zip(client_weights, wtarray):\n",
        "            decrypted_layer_weights = []\n",
        "            flat_array = layer.astype(np.float64).flatten()\n",
        "            chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
        "            for chunk, encrypted_chunk in zip(chunks, layer_weights):\n",
        "                decrypted_chunk = HE.decryptFrac(encrypted_chunk)\n",
        "                original_chunk_size = len(chunk)\n",
        "                decrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
        "                decrypted_layer_weights.append(decrypted_chunk)\n",
        "            decrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
        "            decrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
        "            decrypted_client_weights.append(decrypted_layer_weights)\n",
        "        decrypted_weights.append(decrypted_client_weights)\n",
        "    return decrypted_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TcJH5SaP-f8m"
      },
      "outputs": [],
      "source": [
        "accuracies = [[] for _ in range(len(clients))]\n",
        "losses = [[] for _ in range(len(clients))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uoBGjGa4-f8n"
      },
      "outputs": [],
      "source": [
        "# def train_model(model, x_train, y_train):\n",
        "#     meta_updates = []\n",
        "#     accuracy_over_time = []\n",
        "#     for meta_update in range(num_meta_updates):\n",
        "#         # Sample a meta-batch of tasks\n",
        "#         meta_batch = tf.random.shuffle(tf.range(len(x_train)))[:meta_batch_size]\n",
        "\n",
        "#         # Inner loop updates for each task\n",
        "#         for task in meta_batch:\n",
        "#             task_data = (\n",
        "#                 x_train[task : task + inner_batch_size],\n",
        "#                 y_train[task : task + inner_batch_size],\n",
        "#             )\n",
        "\n",
        "#             for inner_update in range(num_inner_updates):\n",
        "#                 model.train_step(task_data)\n",
        "\n",
        "#         # Evaluate on the meta-test set\n",
        "#         _, accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "#         # Store the meta-update step and accuracy\n",
        "#         meta_updates.append(meta_update + 1)\n",
        "#         accuracy_over_time.append(accuracy)\n",
        "#     avg_accuracy = sum(accuracy_over_time) / len(accuracy_over_time)\n",
        "#     return model , avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zY7wUsss-f8n"
      },
      "outputs": [],
      "source": [
        "meta_batch_size = 32  # Number of tasks per meta-update\n",
        "inner_batch_size = 5  # Number of examples per task\n",
        "num_inner_updates = 5  # Number of inner loop updates per task\n",
        "num_meta_updates = 1\n",
        "optimizer = tf.keras.optimizers.legacy.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "35eivoQU-f8n"
      },
      "outputs": [],
      "source": [
        "def train_model(model, x_train, y_train):\n",
        "    meta_updates = []\n",
        "    accuracy_over_time = []\n",
        "    for meta_update in range(num_meta_updates):\n",
        "        # Sample a meta-batch of tasks\n",
        "        meta_batch = tf.random.shuffle(tf.range(len(x_train)))[:meta_batch_size]\n",
        "\n",
        "        # Inner loop updates for each task\n",
        "        task_gradients = []\n",
        "        for task in meta_batch:\n",
        "            task_data = (\n",
        "                x_train[task : task + inner_batch_size],\n",
        "                y_train[task : task + inner_batch_size],\n",
        "            )\n",
        "\n",
        "            with tf.GradientTape() as outer_tape:\n",
        "                with tf.GradientTape() as inner_tape:\n",
        "                    # Forward pass on the task-specific data\n",
        "                    predictions = model(task_data[0])\n",
        "                    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                        task_data[1], predictions\n",
        "                    )\n",
        "\n",
        "                # Compute gradients for inner loop update\n",
        "                inner_gradients = inner_tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "                # Apply inner loop update to the model's variables\n",
        "                inner_model = MAML(create_model())\n",
        "                inner_model.set_weights(model.get_weights())\n",
        "                optimizer.apply_gradients(\n",
        "                    zip(inner_gradients, inner_model.trainable_variables)\n",
        "                )\n",
        "\n",
        "                # Forward pass with the updated model\n",
        "                updated_predictions = inner_model(task_data[0])\n",
        "                updated_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                    task_data[1], updated_predictions\n",
        "                )\n",
        "\n",
        "            # Compute gradients for outer loop update\n",
        "            outer_gradients = outer_tape.gradient(\n",
        "                updated_loss, model.trainable_variables\n",
        "            )\n",
        "            task_gradients.append(outer_gradients)\n",
        "\n",
        "        # Filter out None gradients\n",
        "        filtered_task_gradients = [\n",
        "            [grad for grad in task_grad if grad is not None]\n",
        "            for task_grad in task_gradients\n",
        "        ]\n",
        "\n",
        "        # Compute average gradients across tasks\n",
        "        avg_gradients = [\n",
        "            tf.reduce_mean(grad_list, axis=0)\n",
        "            for grad_list in zip(*filtered_task_gradients)\n",
        "        ]\n",
        "\n",
        "        # Apply outer loop update to the model's variables\n",
        "        optimizer.apply_gradients(zip(avg_gradients, model.trainable_variables))\n",
        "\n",
        "        # Evaluate on the meta-test set\n",
        "        _, accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "        # Store the meta-update step and accuracy\n",
        "        meta_updates.append(meta_update + 1)\n",
        "        accuracy_over_time.append(accuracy)\n",
        "\n",
        "    avg_accuracy = sum(accuracy_over_time) / len(accuracy_over_time)\n",
        "    return model, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwnrIMdq-f8n",
        "outputId": "f55d0a45-cc4f-4590-80d6-7949179aa6ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:01<?, ?it/s]2024-05-31 18:36:57.474754: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: JIT compilation failed.\n",
            "\n"
          ]
        },
        {
          "ename": "UnknownError",
          "evalue": "{{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log] name: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m wt,model , dataset , i , \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(wts, models, dataset_parts , \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(clients))):\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_weights(wt)\n\u001b[0;32m----> 8\u001b[0m     model, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# history = model.fit(dataset[0], dataset[1], epochs=1,  verbose=1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(history.history[\"accuracy\"][0], history.history[\"loss\"][0])\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# accuracies[i ].append(history.history[\"accuracy\"][0])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# losses[i].append(history.history[\"loss\"][0])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
            "Cell \u001b[0;32mIn[43], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m inner_tape:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Forward pass on the task-specific data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(task_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compute gradients for inner loop update\u001b[39;00m\n\u001b[1;32m     25\u001b[0m inner_gradients \u001b[38;5;241m=\u001b[39m inner_tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
            "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/keras/src/losses.py:2454\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[1;32m   2404\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.metrics.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2409\u001b[0m     y_true, y_pred, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ignore_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m ):\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sparse categorical crossentropy loss.\u001b[39;00m\n\u001b[1;32m   2412\u001b[0m \n\u001b[1;32m   2413\u001b[0m \u001b[38;5;124;03m    Standalone usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[38;5;124;03m        Sparse categorical crossentropy loss value.\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/keras/src/backend.py:5735\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   5733\u001b[0m     epsilon_ \u001b[38;5;241m=\u001b[39m _constant_to_tensor(epsilon(), output\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype)\n\u001b[1;32m   5734\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(output, epsilon_, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon_)\n\u001b[0;32m-> 5735\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5737\u001b[0m \u001b[38;5;66;03m# Permute output so that the last axis contains the logits/probabilities.\u001b[39;00m\n\u001b[1;32m   5738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
            "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log] name: "
          ]
        }
      ],
      "source": [
        "cwts = [encrypt_wt(model.get_weights() , i) for i , model in enumerate(models)]\n",
        "for e in tqdm(range(epochs)):\n",
        "    cwts = aggregate_wt(cwts)\n",
        "    wts = decrypt_weights(cwts)\n",
        "    cwts = []\n",
        "    for wt,model , dataset , i , in zip(wts, models, dataset_parts , range(len(clients))):\n",
        "        model.set_weights(wt)\n",
        "        model, accuracy = train_model(model, dataset[0], dataset[1])\n",
        "        # history = model.fit(dataset[0], dataset[1], epochs=1,  verbose=1)\n",
        "        # print(history.history[\"accuracy\"][0], history.history[\"loss\"][0])\n",
        "        # accuracies[i ].append(history.history[\"accuracy\"][0])\n",
        "        # losses[i].append(history.history[\"loss\"][0])\n",
        "        accuracies.append(accuracy)\n",
        "        print(\"accuracies\" , accuracy)\n",
        "        wtarray = model.get_weights()\n",
        "        cwts.append(encrypt_wt(wtarray , i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "php89Ijg-f8n"
      },
      "outputs": [],
      "source": [
        "accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s4vyanE-f8n"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, client in enumerate(clients):\n",
        "    plt.plot(\n",
        "        epochs_range,\n",
        "        accuracies[i],\n",
        "        label=f\"Client {client}\" if client != 0 else \"aggregate\",\n",
        "    )\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy for Each Client\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, client in enumerate(clients):\n",
        "    plt.plot(epochs_range, losses[i], label=f\"Client {client}\" if client != 0 else \"aggregate\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss for Each Client\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyrbTbPh-f8n"
      },
      "outputs": [],
      "source": [
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# for i, client in enumerate(clients):\n",
        "plt.plot(epochs_range, accuracies[0], label=f\"Client {client}\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy for Each Client\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXs974ry-f8n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = models[0]\n",
        "# Assuming you have a trained model named 'model'\n",
        "# and input data 'X_test' and corresponding labels 'y_test'\n",
        "\n",
        "# Select a sample image from the test set\n",
        "# Select a sample image from the test set\n",
        "sample_image = X_test[1]  # Adjust the index as needed\n",
        "sample_label = y_test[1]\n",
        "\n",
        "# Preprocess the sample image\n",
        "sample_image = sample_image[np.newaxis, ...]  # Add batch dimension\n",
        "\n",
        "# Initialize the model\n",
        "# model = model_init()\n",
        "\n",
        "# Create a model that outputs the activations of the first dense layer\n",
        "layer_name = \"dense_40\"  # Name of the first dense layer in your model\n",
        "activation_model = Model(\n",
        "    inputs=model.inputs, outputs=model.get_layer(layer_name).output\n",
        ")\n",
        "\n",
        "# Get the activations of the first dense layer\n",
        "activations = activation_model.predict(sample_image)\n",
        "\n",
        "# Plot the sample image and the activation map\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax1.imshow(sample_image[0], cmap=\"gray\")  # Assuming grayscale image\n",
        "ax1.set_title(\"Sample Image\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "ax2.imshow(activations[0].reshape(2, 4), cmap=\"viridis\", interpolation=\"nearest\")\n",
        "ax2.set_title(\"Activation Map\")\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "he38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

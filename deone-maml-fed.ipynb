{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of GPUs available: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3090\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available.\")\n",
    "        print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "current_timestamp = datetime.now()\n",
    "\n",
    "# Format the timestamp in a human-readable form\n",
    "folder_path = current_timestamp.strftime(\"%d_%H_%M\")\n",
    "fp = f\"models/{folder_path}\"\n",
    "if not os.path.exists(fp):\n",
    "\tos.makedirs(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"drone/class_dict_seg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [0 , 1 , 2]\n",
    "no_clients = len(clients)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"drone/dataset/semantic_drone_dataset/label_images_semantic\"\n",
    "original_image_path = \"drone/dataset/semantic_drone_dataset/original_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(image_path)\n",
    "\n",
    "# Filter out only the image files (assuming image files have extensions like .jpg, .png, etc.)\n",
    "image_files = [\n",
    "\tfile for file in files if file.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"))\n",
    "]\n",
    "image_files = sorted(image_files)\n",
    "\n",
    "# list o image_paths\n",
    "image_list = []\n",
    "# Print the list of image files\n",
    "for image_file in image_files:\n",
    "\timage_list.append(image_path + \"/\" + image_file)\n",
    "\n",
    "\n",
    "files_2 = os.listdir(original_image_path)\n",
    "# Filter out only the image files (assuming image files have extensions like .jpg, .png, etc.)\n",
    "image_files = [\n",
    "\tfile for file in files_2 if file.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"))\n",
    "]\n",
    "image_files = sorted(image_files)\n",
    "\n",
    "# list o image_paths\n",
    "original_image_list = []\n",
    "# Print the list of image files\n",
    "for image_file in image_files:\n",
    "\toriginal_image_list.append(original_image_path + \"/\" + image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"drone/dataset/semantic_drone_dataset/original_images\"\n",
    "MASK_PATH = \"drone/dataset/semantic_drone_dataset/label_images_semantic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images:  400\n"
     ]
    }
   ],
   "source": [
    "n_classes = 23\n",
    "\n",
    "\n",
    "def create_df():\n",
    "\tname = []\n",
    "\tfor dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "\t\tfor filename in filenames:\n",
    "\t\t\tname.append(filename.split(\".\")[0])\n",
    "\n",
    "\treturn pd.DataFrame({\"id\": name}, index=np.arange(0, len(name)))\n",
    "\n",
    "\n",
    "df = create_df()\n",
    "print(\"Total Images: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id    515\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data for traning , testing and validation\n",
    "X_trainval, X_test = train_test_split(df[\"id\"].values, test_size=0.1, random_state=42)\n",
    "X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = [X_train[i*len(X_train)//no_clients:(i+1)*len(X_train)//no_clients] for i in range(no_clients)]\n",
    "split_val = [X_val[i*len(X_val)//no_clients:(i+1)*len(X_val)//no_clients] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "\t\tself.img_path = img_path\n",
    "\t\tself.mask_path = mask_path\n",
    "\t\tself.X = X\n",
    "\t\tself.transform = transform\n",
    "\t\tself.patches = patch\n",
    "\t\tself.mean = mean\n",
    "\t\tself.std = std\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_full_path = os.path.join(self.img_path, self.X[idx] + \".jpg\")\n",
    "\t\tmask_full_path = os.path.join(self.mask_path, self.X[idx] + \".png\")\n",
    "\n",
    "\t\t# Load the image and mask\n",
    "\t\timg = cv2.imread(img_full_path)\n",
    "\t\tif img is None:\n",
    "\t\t\traise FileNotFoundError(f\"Image not found at {img_full_path}\")\n",
    "\n",
    "\t\tmask = cv2.imread(mask_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "\t\tif mask is None:\n",
    "\t\t\traise FileNotFoundError(f\"Mask not found at {mask_full_path}\")\n",
    "\n",
    "\t\t# Convert color from BGR to RGB\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\taug = self.transform(image=img, mask=mask)\n",
    "\t\t\timg = Image.fromarray(aug[\"image\"])\n",
    "\t\t\tmask = aug[\"mask\"]\n",
    "\n",
    "\t\tif self.transform is None:\n",
    "\t\t\timg = Image.fromarray(img)\n",
    "\n",
    "\t\tt = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "\t\timg = t(img)\n",
    "\t\tmask = torch.from_numpy(mask).long()\n",
    "\n",
    "\t\tif self.patches:\n",
    "\t\t\timg, mask = self.tiles(img, mask)\n",
    "\n",
    "\t\treturn img, mask\n",
    "\n",
    "\tdef tiles(self, img, mask):\n",
    "\n",
    "\t\timg_patches = img.unfold(1, 512, 512).unfold(2, 768, 768)\n",
    "\t\timg_patches = img_patches.contiguous().view(3, -1, 512, 768)\n",
    "\t\timg_patches = img_patches.permute(1, 0, 2, 3)\n",
    "\n",
    "\t\tmask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "\t\tmask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "\n",
    "\t\treturn img_patches, mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = A.Compose(\n",
    "\t[\n",
    "\t\tA.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "\t\tA.HorizontalFlip(),\n",
    "\t\tA.VerticalFlip(),\n",
    "\t\tA.GridDistortion(p=0.2),\n",
    "\t\tA.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
    "\t\tA.GaussNoise(),\n",
    "\t]\n",
    ")\n",
    "\n",
    "t_val = A.Compose(\n",
    "\t[\n",
    "\t\tA.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "\t\tA.HorizontalFlip(),\n",
    "\t\tA.GridDistortion(p=0.2),\n",
    "\t]\n",
    ")\n",
    "\n",
    "# datasets\n",
    "train_set = DroneDataset(\n",
    "\tIMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False\n",
    ")\n",
    "val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
    "\n",
    "# dataloader\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_loaders = []\n",
    "val_loaders = [] \n",
    "for i in range(no_clients):\n",
    "\tb_train = split_train[i]\n",
    "\tb_val = split_val[i]\n",
    "\ttrain_set = DroneDataset(\n",
    "\t\tIMAGE_PATH, MASK_PATH, b_train, mean, std, t_train, patch=False\n",
    "\t)\n",
    "\tval_set = DroneDataset(IMAGE_PATH, MASK_PATH, b_val, mean, std, t_val, patch=False)\n",
    "\n",
    "\t# dataloader\n",
    "\tbatch_size = 1\n",
    "\n",
    "\ttrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\tval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\ttrain_loaders.append(train_loader)\n",
    "\tval_loaders.append(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(no_clients):\n",
    "\tmodel = smp.Unet(\n",
    "\t\tencoder_name=\"resnet34\",\n",
    "\t\tencoder_weights=\"imagenet\",\n",
    "\t\tin_channels=3,\n",
    "\t\tclasses=n_classes,\n",
    "\t)\n",
    "\tmodels.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "\t\tcorrect = torch.eq(output, mask).int()\n",
    "\t\taccuracy = float(correct.sum()) / float(correct.numel())\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "\twith torch.no_grad():\n",
    "\t\tpred_mask = F.softmax(pred_mask, dim=1)\n",
    "\t\tpred_mask = torch.argmax(pred_mask, dim=1)\n",
    "\t\tpred_mask = pred_mask.contiguous().view(-1)\n",
    "\t\tmask = mask.contiguous().view(-1)\n",
    "\n",
    "\t\tiou_per_class = []\n",
    "\t\tfor clas in range(0, n_classes):  # loop per pixel class\n",
    "\t\t\ttrue_class = pred_mask == clas\n",
    "\t\t\ttrue_label = mask == clas\n",
    "\n",
    "\t\t\tif true_label.long().sum().item() == 0:  # no exist label in this loop\n",
    "\t\t\t\tiou_per_class.append(np.nan)\n",
    "\t\t\telse:\n",
    "\t\t\t\tintersect = (\n",
    "\t\t\t\t\ttorch.logical_and(true_class, true_label).sum().float().item()\n",
    "\t\t\t\t)\n",
    "\t\t\t\tunion = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "\t\t\t\tiou = (intersect + smooth) / (union + smooth)\n",
    "\t\t\t\tiou_per_class.append(iou)\n",
    "\t\treturn np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot(pred_masks, true_masks):\n",
    "\tn_classes = len(np.unique(true_masks))  # Number of unique classes\n",
    "\tclass_colors = plt.cm.tab20.colors  # Colors for different classes\n",
    "\n",
    "\tn_images = pred_masks.shape[0]  # Number of images\n",
    "\n",
    "\t# Create subplots outside the loop\n",
    "\tfig, axes = plt.subplots(n_images, 2, figsize=(10, 5 * n_images))\n",
    "\n",
    "\tfor i in range(n_images):\n",
    "\t\tim_pred = axes[i, 0].imshow(\n",
    "\t\t\tpred_masks[i, 0], cmap=\"tab20\", vmin=0, vmax=n_classes - 1\n",
    "\t\t)  # Assuming single channel masks\n",
    "\t\taxes[i, 0].set_title(\"Predicted Mask\")\n",
    "\t\taxes[i, 0].set_axis_off()\n",
    "\t\tfig.colorbar(im_pred, ax=axes[i, 0], label=\"Predicted Class\")\n",
    "\n",
    "\t\tim_true = axes[i, 1].imshow(\n",
    "\t\t\ttrue_masks[i], cmap=\"tab20\", vmin=0, vmax=n_classes - 1\n",
    "\t\t)\n",
    "\t\taxes[i, 1].set_title(\"True Mask\")\n",
    "\t\taxes[i, 1].set_axis_off()\n",
    "\t\tfig.colorbar(im_true, ax=axes[i, 1], label=\"True Class\")\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import learn2learn as l2l\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\treturn param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def fit(\n",
    "\tepochs,\n",
    "\tmodel,\n",
    "\ttrain_loader,\n",
    "\tval_loader,\n",
    "\tcriterion,\n",
    "\toptimizer,\n",
    "\tscheduler,\n",
    "\tpatch=False,\n",
    "\tadaptation_steps=5,\n",
    "\tinner_lr=0.01,\n",
    "):\n",
    "\ttorch.cuda.empty_cache()\n",
    "\ttrain_losses = []\n",
    "\ttest_losses = []\n",
    "\tval_iou = []\n",
    "\tval_acc = []\n",
    "\ttrain_iou = []\n",
    "\ttrain_acc = []\n",
    "\tlrs = []\n",
    "\tmin_loss = np.inf\n",
    "\tdecrease = 1\n",
    "\tnot_improve = 0\n",
    "\n",
    "\tmodel.to(device)\n",
    "\tfit_time = time.time()\n",
    "\tfor e in range(epochs):\n",
    "\t\tsince = time.time()\n",
    "\t\trunning_loss = 0\n",
    "\t\tiou_score = 0\n",
    "\t\taccuracy = 0\n",
    "\n",
    "\t\t# training loop\n",
    "\t\tmodel.train()\n",
    "\t\tfor i, data in enumerate(tqdm(train_loader)):\n",
    "\t\t\t# training phase\n",
    "\t\t\timage_tiles, mask_tiles = data\n",
    "\t\t\tif patch:\n",
    "\t\t\t\tbs, n_tiles, c, h, w = image_tiles.size()\n",
    "\t\t\t\timage_tiles = image_tiles.view(-1, c, h, w)\n",
    "\t\t\t\tmask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "\t\t\timage = image_tiles.to(device)\n",
    "\t\t\tmask = mask_tiles.to(device)\n",
    "\n",
    "\t\t\t# Meta-learning with MAML\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# Clone the model for inner-loop updates\n",
    "\t\t\tlearner = l2l.algorithms.MAML(model, lr=inner_lr).clone()\n",
    "\n",
    "\t\t\t# Inner loop: Adaptation\n",
    "\t\t\tfor step in range(adaptation_steps):\n",
    "\t\t\t\toutput = learner(image)\n",
    "\t\t\t\tloss = criterion(output, mask)\n",
    "\t\t\t\tlearner.adapt(loss)\n",
    "\n",
    "\t\t\t# Meta-update\n",
    "\t\t\toutput = learner(image)\n",
    "\t\t\tloss = criterion(output, mask)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Step the learning rate\n",
    "\t\t\tlrs.append(get_lr(optimizer))\n",
    "\t\t\tscheduler.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\t\t# Evaluation metrics\n",
    "\t\t\tiou_score += mIoU(output, mask)\n",
    "\t\t\taccuracy += pixel_accuracy(output, mask)\n",
    "\n",
    "\t\t# Validation loop\n",
    "\t\tmodel.eval()\n",
    "\t\ttest_loss = 0\n",
    "\t\ttest_accuracy = 0\n",
    "\t\tval_iou_score = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i, data in enumerate(tqdm(val_loader)):\n",
    "\t\t\t\timage_tiles, mask_tiles = data\n",
    "\t\t\t\tif patch:\n",
    "\t\t\t\t\tbs, n_tiles, c, h, w = image_tiles.size()\n",
    "\t\t\t\t\timage_tiles = image_tiles.view(-1, c, h, w)\n",
    "\t\t\t\t\tmask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "\t\t\t\timage = image_tiles.to(device)\n",
    "\t\t\t\tmask = mask_tiles.to(device)\n",
    "\n",
    "\t\t\t\toutput = model(image)\n",
    "\t\t\t\tval_iou_score += mIoU(output, mask)\n",
    "\t\t\t\ttest_accuracy += pixel_accuracy(output, mask)\n",
    "\t\t\t\tloss = criterion(output, mask)\n",
    "\t\t\t\ttest_loss += loss.item()\n",
    "\n",
    "\t\t# Calculate mean for each batch\n",
    "\t\ttrain_losses.append(running_loss / len(train_loader))\n",
    "\t\ttest_losses.append(test_loss / len(val_loader))\n",
    "\n",
    "\t\tif min_loss > (test_loss / len(val_loader)):\n",
    "\t\t\tprint(\n",
    "\t\t\t\t\"Loss Decreasing.. {:.3f} >> {:.3f} \".format(\n",
    "\t\t\t\t\tmin_loss, (test_loss / len(val_loader))\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\tmin_loss = test_loss / len(val_loader)\n",
    "\t\t\tdecrease += 1\n",
    "\t\t\tif decrease % 5 == 0:\n",
    "\t\t\t\tprint(\"saving model...\")\n",
    "\t\t\t\ttorch.save(\n",
    "\t\t\t\t\tmodel,\n",
    "\t\t\t\t\t\"Unet-Mobilenet_v2_mIoU-{:.3f}.pt\".format(\n",
    "\t\t\t\t\t\tval_iou_score / len(val_loader)\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tif (test_loss / len(val_loader)) > min_loss:\n",
    "\t\t\tnot_improve += 1\n",
    "\t\t\tmin_loss = test_loss / len(val_loader)\n",
    "\t\t\tprint(f\"Loss Not Decrease for {not_improve} time\")\n",
    "\t\t\tif not_improve == 7:\n",
    "\t\t\t\tprint(\"Loss not decrease for 7 times, Stop Training\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t# Update metrics\n",
    "\t\tval_iou.append(val_iou_score / len(val_loader))\n",
    "\t\ttrain_iou.append(iou_score / len(train_loader))\n",
    "\t\ttrain_acc.append(accuracy / len(train_loader))\n",
    "\t\tval_acc.append(test_accuracy / len(val_loader))\n",
    "\t\tprint(\n",
    "\t\t\t\"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "\t\t\t\"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "\t\t\t\"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "\t\t\t\"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
    "\t\t\t\"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
    "\t\t\t\"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
    "\t\t\t\"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
    "\t\t\t\"Time: {:.2f}m\".format((time.time() - since) / 60),\n",
    "\t\t)\n",
    "\n",
    "\thistory = {\n",
    "\t\t\"train_loss\": train_losses,\n",
    "\t\t\"val_loss\": test_losses,\n",
    "\t\t\"train_miou\": train_iou,\n",
    "\t\t\"val_miou\": val_iou,\n",
    "\t\t\"train_acc\": train_acc,\n",
    "\t\t\"val_acc\": val_acc,\n",
    "\t\t\"lrs\": lrs,\n",
    "\t}\n",
    "\tprint(\"Total time: {:.2f} m\".format((time.time() - fit_time) / 60))\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lr = 1e-3\n",
    "# epoch = 2\n",
    "# weight_decay = 1e-4\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer, max_lr, epochs=epoch, steps_per_epoch=len(train_loader)\n",
    "# )\n",
    "\n",
    "# history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_train(model):\n",
    "\thistory = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)\n",
    "\treturn model , history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "HE = Pyfhel()\n",
    "ckks_params = {\n",
    "\t\"scheme\": \"CKKS\",\n",
    "\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "}\n",
    "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "HE.rotateKeyGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffie_hellman_parameters():\n",
    "\tparameters = dh.generate_parameters(generator=2, key_size=512)\n",
    "\treturn parameters\n",
    "\n",
    "\n",
    "def generate_diffie_hellman_keys(parameters):\n",
    "\tprivate_key = parameters.generate_private_key()\n",
    "\tpublic_key = private_key.public_key()\n",
    "\treturn private_key, public_key\n",
    "\n",
    "\n",
    "def derive_key(private_key, peer_public_key):\n",
    "\tshared_key = private_key.exchange(peer_public_key)\n",
    "\tderived_key = HKDF(\n",
    "\t\talgorithm=hashes.SHA256(),\n",
    "\t\tlength=32,\n",
    "\t\tsalt=None,\n",
    "\t\tinfo=b\"handshake data\",\n",
    "\t).derive(shared_key)\n",
    "\treturn derived_key\n",
    "\n",
    "\n",
    "def encrypt_message_AES(key, message):\n",
    "\tserialized_obj = pickle.dumps(message)\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.ECB())\n",
    "\tencryptor = cipher.encryptor()\n",
    "\tpadded_obj = serialized_obj + b\" \" * (16 - len(serialized_obj) % 16)\n",
    "\tciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "\treturn ciphertext\n",
    "\n",
    "\n",
    "def decrypt_message_AES(key, ciphertext):\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.ECB())\n",
    "\tdecryptor = cipher.decryptor()\n",
    "\tpadded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "\tserialized_obj = padded_obj.rstrip(b\" \")\n",
    "\tobj = pickle.loads(serialized_obj)\n",
    "\treturn obj\n",
    "\n",
    "\n",
    "def setup_AES():\n",
    "\tnum_clients = len(clients)\n",
    "\tparameters = generate_diffie_hellman_parameters()\n",
    "\tserver_private_key, server_public_key = generate_diffie_hellman_keys(parameters)\n",
    "\tclient_keys = [generate_diffie_hellman_keys(parameters) for _ in range(num_clients)]\n",
    "\tshared_keys = [\n",
    "\t\tderive_key(server_private_key, client_public_key)\n",
    "\t\tfor _, client_public_key in client_keys\n",
    "\t]\n",
    "\tclient_shared_keys = [\n",
    "\t\tderive_key(client_private_key, server_public_key)\n",
    "\t\tfor client_private_key, _ in client_keys\n",
    "\t]\n",
    "\n",
    "\treturn client_keys, shared_keys, client_shared_keys\n",
    "\n",
    "client_keys, shared_keys, client_shared_keys = setup_AES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights):\n",
    "\twith torch.no_grad(): \n",
    "\t\tfor param, weight in zip(model.parameters(), weights):\n",
    "\t\t\tparam.copy_(torch.tensor(weight))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model):\n",
    "\treturn [param.cpu().detach().numpy() for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wt(encypted_cwts):\n",
    "\t# cwts = []\n",
    "\t# for i, ecwt in enumerate(encypted_cwts):\n",
    "\t# \tcwts.append(decrypt_message_AES(shared_keys[i], ecwt))\n",
    "\tcwts = encypted_cwts\n",
    "\tresmodel = []\n",
    "\tfor j in range(len(cwts[0])):  # for layers\n",
    "\t\tlayer = []\n",
    "\t\tfor k in range(len(cwts[0][j])):  # for chunks\n",
    "\t\t\ttmp = cwts[0][j][k].copy()\n",
    "\t\t\tfor i in range(1, len(cwts)):  # for clients\n",
    "\t\t\t\ttmp = tmp + cwts[i][j][k]\n",
    "\t\t\ttmp = tmp / len(cwts)\n",
    "\t\t\tlayer.append(tmp)\n",
    "\t\tresmodel.append(layer)\n",
    "\n",
    "\tres = [resmodel.copy() for _ in range(len(clients))]\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_wt(wtarray, i):\n",
    "\tcwt = []\n",
    "\tfor layer in wtarray:\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\t\tclayer = []\n",
    "\t\tfor chunk in chunks:\n",
    "\t\t\tptxt = HE.encodeFrac(chunk)\n",
    "\t\t\tctxt = HE.encryptPtxt(ptxt)\n",
    "\t\t\tclayer.append(ctxt)\n",
    "\t\tcwt.append(clayer.copy())\n",
    "\t# ciphertext = encrypt_message_AES(client_shared_keys[i], cwt)\n",
    "\t# return ciphertext\n",
    "\treturn cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_weights(res):\n",
    "\tdecrypted_weights = []\n",
    "\tfor client_weights, model in zip(res, models):\n",
    "\t\tdecrypted_client_weights = []\n",
    "\t\twtarray = get_weights(model)\n",
    "\t\tfor layer_weights, layer in zip(client_weights, wtarray):\n",
    "\t\t\tdecrypted_layer_weights = []\n",
    "\t\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\t\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\t\t\tfor chunk, encrypted_chunk in zip(chunks, layer_weights):\n",
    "\t\t\t\tdecrypted_chunk = HE.decryptFrac(encrypted_chunk)\n",
    "\t\t\t\toriginal_chunk_size = len(chunk)\n",
    "\t\t\t\tdecrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "\t\t\t\tdecrypted_layer_weights.append(decrypted_chunk)\n",
    "\t\t\tdecrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
    "\t\t\tdecrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
    "\t\t\tdecrypted_client_weights.append(decrypted_layer_weights)\n",
    "\t\tdecrypted_weights.append(decrypted_client_weights)\n",
    "\treturn decrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 0.01\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories = []\n",
    "# cwts = [encrypt_wt(get_weights(model), i) for i, model in enumerate(models)]\n",
    "# print(\"Initial encrypted weights generated for all clients.\")\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     print(f\"Epoch {e+1}/{epochs} started\")\n",
    "\t\n",
    "#     cwts = aggregate_wt(cwts)\n",
    "#     print(f\"Aggregated encrypted weights after epoch {e+1}\")\n",
    "\t\n",
    "#     wts = decrypt_weights(cwts)\n",
    "#     print(f\"Decrypted weights after aggregation for epoch {e+1}\")\n",
    "\t\n",
    "#     cwts = []\n",
    "#     epoch_histories = []\n",
    "\t\n",
    "#     for i in range(no_clients):\n",
    "#         print(f\"Client {i} preparing to load weights and datasets for epoch {e+1}\")\n",
    "\t\t\n",
    "#         wt = wts[i]\n",
    "#         model = load_weights(models[i], wt)\n",
    "\t\t\n",
    "#         train_loader = train_loaders[i]\n",
    "#         val_loader = val_loaders[i]\n",
    "\t\t\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "#         sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=1, steps_per_epoch=len(train_loader))\n",
    "\t\t\n",
    "#         print(f\"Client {i} Epoch {e+1} started\")\n",
    "\t\t\n",
    "#         history = fit(1, model, train_loader, val_loader, criterion, optimizer, sched)\n",
    "\t\t\n",
    "#         epoch_histories.append(history)\n",
    "#         print(f\"Client {i} Epoch {e+1} completed with history: {history}\")\n",
    "\t\t\n",
    "#         wtarray = get_weights(model)\n",
    "#         cwts.append(encrypt_wt(wtarray, i))\n",
    "#         print(f\"Client {i} weights encrypted and added to cwts for epoch {e+1}\")\n",
    "\n",
    "#     histories.append(epoch_histories)\n",
    "#     print(f\"Epoch {e+1} completed and histories updated\")\n",
    "\n",
    "# print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# histories = []\n",
    "# previous_losses = {i: [] for i in range(no_clients)}\n",
    "\n",
    "# cwts = [encrypt_wt(get_weights(model), i) for i, model in enumerate(models)]\n",
    "# print(\"Initial encrypted weights generated for all clients.\")\n",
    "\n",
    "# for e in tqdm(range(epochs), desc=\"Epochs\", colour='green'):\n",
    "#     print(f\"Epoch {e+1}/{epochs} started\")\n",
    "\t\n",
    "#     cwts = aggregate_wt(cwts)\n",
    "#     print(f\"Aggregated encrypted weights after epoch {e+1}\")\n",
    "\t\n",
    "#     wts = decrypt_weights(cwts)\n",
    "#     print(f\"Decrypted weights after aggregation for epoch {e+1}\")\n",
    "\t\n",
    "#     cwts = []\n",
    "#     epoch_histories = []\n",
    "\t\n",
    "#     for i in range(no_clients):\n",
    "#         print(f\"Client {i} preparing to load weights and datasets for epoch {e+1}\")\n",
    "\t\t\n",
    "#         wt = wts[i]\n",
    "#         model = load_weights(models[i], wt)\n",
    "\t\t\n",
    "#         train_loader = train_loaders[i]\n",
    "#         val_loader = val_loaders[i]\n",
    "\t\t\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "#         sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=1, steps_per_epoch=len(train_loader))\n",
    "\t\t\n",
    "#         print(f\"Client {i} previous losses before epoch {e+1}: {previous_losses[i]}\")\n",
    "#         print(f\"Client {i} Epoch {e+1} started\")\n",
    "\t\t\n",
    "#         history = fit(1, model, tqdm(train_loader, desc=f\"Client {i} Training\", colour='blue'), val_loader, criterion, optimizer, sched)\n",
    "\t\t\n",
    "#         epoch_histories.append(history)\n",
    "#         print(f\"Client {i} Epoch {e+1} completed with history: {history}\")\n",
    "\t\t\n",
    "#         previous_losses[i].append({\n",
    "#             'train_loss': history['train_loss'][-1],\n",
    "#             'val_loss': history['val_loss'][-1]\n",
    "#         })\n",
    "#         print(f\"Client {i} previous losses updated: {previous_losses[i]}\")\n",
    "\t\t\n",
    "#         wtarray = get_weights(model)\n",
    "#         cwts.append(encrypt_wt(wtarray, i))\n",
    "#         print(f\"Client {i} weights encrypted and added to cwts for epoch {e+1}\")\n",
    "\n",
    "#     histories.append(epoch_histories)\n",
    "#     print(f\"Epoch {e+1} completed and histories updated\")\n",
    "\n",
    "# print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting accuracy over time\n",
    "# train_accuracies = {i: [] for i in range(no_clients)}\n",
    "# val_accuracies = {i: [] for i in range(no_clients)}\n",
    "\n",
    "# for epoch_histories in histories:\n",
    "#     for i, history in enumerate(epoch_histories):\n",
    "#         train_accuracies[i].append(history[\"train_accuracy\"][-1])\n",
    "#         val_accuracies[i].append(history[\"val_accuracy\"][-1])\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for i in range(no_clients):\n",
    "#     plt.plot(train_accuracies[i], label=f\"Client {i} Train Accuracy\")\n",
    "#     plt.plot(val_accuracies[i], label=f\"Client {i} Val Accuracy\")\n",
    "\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Training and Validation Accuracy Over Time\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for e, epoch_histories in enumerate(histories):\n",
    "#     print(f\"Epoch {e+1} histories:\")\n",
    "#     for i, history in enumerate(epoch_histories):\n",
    "#         print(f\"  Client {i}: {history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "\tepochs,\n",
    "\tmodel,\n",
    "\ttrain_loader,\n",
    "\tval_loader,\n",
    "\tcriterion,\n",
    "\toptimizer,\n",
    "\tscheduler,\n",
    "\tpatch=False,\n",
    "\tadaptation_steps=5,\n",
    "\tinner_lr=0.01,\n",
    "):\n",
    "\ttorch.cuda.empty_cache()\n",
    "\ttrain_losses = []\n",
    "\ttest_losses = []\n",
    "\tval_iou = []\n",
    "\tval_acc = []\n",
    "\ttrain_iou = []\n",
    "\ttrain_acc = []\n",
    "\tlrs = []\n",
    "\tmin_loss = np.inf\n",
    "\tdecrease = 1\n",
    "\tnot_improve = 0\n",
    "\n",
    "\tmodel.to(device)\n",
    "\tfit_time = time.time()\n",
    "\tfor e in range(epochs):\n",
    "\t\tsince = time.time()\n",
    "\t\trunning_loss = 0\n",
    "\t\tiou_score = 0\n",
    "\t\taccuracy = 0\n",
    "\n",
    "\t\t# training loop\n",
    "\t\tmodel.train()\n",
    "\t\tfor i, data in enumerate(tqdm(train_loader)):\n",
    "\t\t\t# training phase\n",
    "\t\t\timage_tiles, mask_tiles = data\n",
    "\t\t\tif patch:\n",
    "\t\t\t\tbs, n_tiles, c, h, w = image_tiles.size()\n",
    "\t\t\t\timage_tiles = image_tiles.view(-1, c, h, w)\n",
    "\t\t\t\tmask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "\t\t\timage = image_tiles.to(device)\n",
    "\t\t\tmask = mask_tiles.to(device)\n",
    "\n",
    "\t\t\t# Meta-learning with MAML\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# Clone the model for inner-loop updates\n",
    "\t\t\tlearner = l2l.algorithms.MAML(model, lr=inner_lr).clone()\n",
    "\n",
    "\t\t\t# Inner loop: Adaptation\n",
    "\t\t\tfor step in range(adaptation_steps):\n",
    "\t\t\t\toutput = learner(image)\n",
    "\t\t\t\tloss = criterion(output, mask)\n",
    "\t\t\t\tlearner.adapt(loss)\n",
    "\n",
    "\t\t\t# Meta-update\n",
    "\t\t\toutput = learner(image)\n",
    "\t\t\tloss = criterion(output, mask)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Step the learning rate\n",
    "\t\t\tlrs.append(get_lr(optimizer))\n",
    "\t\t\tscheduler.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\t\t# Evaluation metrics\n",
    "\t\t\tiou_score += mIoU(output, mask)\n",
    "\t\t\taccuracy += pixel_accuracy(output, mask)\n",
    "\n",
    "\t\t# Validation loop\n",
    "\t\tmodel.eval()\n",
    "\t\ttest_loss = 0\n",
    "\t\ttest_accuracy = 0\n",
    "\t\tval_iou_score = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i, data in enumerate(tqdm(val_loader)):\n",
    "\t\t\t\timage_tiles, mask_tiles = data\n",
    "\t\t\t\tif patch:\n",
    "\t\t\t\t\tbs, n_tiles, c, h, w = image_tiles.size()\n",
    "\t\t\t\t\timage_tiles = image_tiles.view(-1, c, h, w)\n",
    "\t\t\t\t\tmask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "\t\t\t\timage = image_tiles.to(device)\n",
    "\t\t\t\tmask = mask_tiles.to(device)\n",
    "\n",
    "\t\t\t\toutput = model(image)\n",
    "\t\t\t\tval_iou_score += mIoU(output, mask)\n",
    "\t\t\t\ttest_accuracy += pixel_accuracy(output, mask)\n",
    "\t\t\t\tloss = criterion(output, mask)\n",
    "\t\t\t\ttest_loss += loss.item()\n",
    "\n",
    "\t\t# Calculate mean for each batch\n",
    "\t\ttrain_losses.append(running_loss / len(train_loader))\n",
    "\t\ttest_losses.append(test_loss / len(val_loader))\n",
    "\n",
    "\t\tif min_loss > (test_loss / len(val_loader)):\n",
    "\t\t\tprint(\n",
    "\t\t\t\t\"Loss Decreasing.. {:.3f} >> {:.3f} \".format(\n",
    "\t\t\t\t\tmin_loss, (test_loss / len(val_loader))\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\tmin_loss = test_loss / len(val_loader)\n",
    "\t\t\tdecrease += 1\n",
    "\t\t\tif decrease % 5 == 0:\n",
    "\t\t\t\tprint(\"saving model...\")\n",
    "\t\t\t\ttorch.save(\n",
    "\t\t\t\t\tmodel,\n",
    "\t\t\t\t\t\"Unet-Mobilenet_v2_mIoU-{:.3f}.pt\".format(\n",
    "\t\t\t\t\t\tval_iou_score / len(val_loader)\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tif (test_loss / len(val_loader)) > min_loss:\n",
    "\t\t\tnot_improve += 1\n",
    "\t\t\tmin_loss = test_loss / len(val_loader)\n",
    "\t\t\tprint(f\"Loss Not Decrease for {not_improve} time\")\n",
    "\t\t\tif not_improve == 7:\n",
    "\t\t\t\tprint(\"Loss not decrease for 7 times, Stop Training\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t# Update metrics\n",
    "\t\tval_iou.append(val_iou_score / len(val_loader))\n",
    "\t\ttrain_iou.append(iou_score / len(train_loader))\n",
    "\t\ttrain_acc.append(accuracy / len(train_loader))\n",
    "\t\tval_acc.append(test_accuracy / len(val_loader))\n",
    "\t\tprint(\n",
    "\t\t\t\"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "\t\t\t\"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "\t\t\t\"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "\t\t\t\"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
    "\t\t\t\"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
    "\t\t\t\"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
    "\t\t\t\"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
    "\t\t\t\"Time: {:.2f}m\".format((time.time() - since) / 60),\n",
    "\t\t)\n",
    "\n",
    "\thistory = {\n",
    "\t\t\"train_loss\": train_losses,\n",
    "\t\t\"val_loss\": test_losses,\n",
    "\t\t\"train_miou\": train_iou,\n",
    "\t\t\"val_miou\": val_iou,\n",
    "\t\t\"train_acc\": train_acc,\n",
    "\t\t\"val_acc\": val_acc,\n",
    "\t\t\"lrs\": lrs,\n",
    "\t}\n",
    "\tprint(\"Total time: {:.2f} m\".format((time.time() - fit_time) / 60))\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial encrypted weights generated for all clients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|\u001b[32m          \u001b[0m| 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 started\n",
      "Aggregated encrypted weights after epoch 1\n",
      "Decrypted weights after aggregation for epoch 1\n",
      "Client 0 preparing for epoch 1\n",
      "Client 0 previous losses: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/home/cs-lab-12/miniconda3/envs/he39/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/cs-lab-12/miniconda3/envs/he39/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Client 0 Training:  74%|\u001b[34m███████▎  \u001b[0m| 75/102 [03:20<01:12,  2.68s/it]\n",
      " 74%|███████▎  | 75/102 [03:20<01:12,  2.68s/it]\n",
      "Epochs:   0%|\u001b[32m          \u001b[0m| 0/100 [03:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 35\u001b[0m\n\u001b[1;32m     30\u001b[0m sched \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(\n\u001b[1;32m     31\u001b[0m \toptimizer, max_lr, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m previous losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_losses[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m Training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolour\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\t\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\t\u001b[49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m epoch_histories\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[1;32m     46\u001b[0m previous_losses[i]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     47\u001b[0m \t{\n\u001b[1;32m     48\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \t}\n\u001b[1;32m     53\u001b[0m )\n",
      "Cell \u001b[0;32mIn[86], line 61\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch, adaptation_steps, inner_lr)\u001b[0m\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m learner(image)\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, mask)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Step the learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/he39/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/he39/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/he39/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = []\n",
    "previous_losses = {i: [] for i in range(no_clients)}\n",
    "\n",
    "cwts = [encrypt_wt(get_weights(model), i) for i, model in enumerate(models)]\n",
    "print(\"Initial encrypted weights generated for all clients.\")\n",
    "\n",
    "for e in tqdm(range(epochs), desc=\"Epochs\", colour=\"green\"):\n",
    "\tprint(f\"Epoch {e+1}/{epochs} started\")\n",
    "\tcwts = aggregate_wt(cwts)\n",
    "\tprint(f\"Aggregated encrypted weights after epoch {e+1}\")\n",
    "\twts = decrypt_weights(cwts)\n",
    "\tprint(f\"Decrypted weights after aggregation for epoch {e+1}\")\n",
    "\n",
    "\tcwts = []\n",
    "\tepoch_histories = []\n",
    "\n",
    "\tfor i in range(no_clients):\n",
    "\t\tprint(f\"Client {i} preparing for epoch {e+1}\")\n",
    "\t\twt = wts[i]\n",
    "\t\tmodel = load_weights(models[i], wt)\n",
    "\t\tif (e % 5 == 0) and i == 0:\n",
    "\t\t\ttorch.save(model, f\"{fp}/{e}_model.pth\")\n",
    "\t\ttrain_loader = train_loaders[i]\n",
    "\t\tval_loader = val_loaders[i]\n",
    "\n",
    "\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\toptimizer = torch.optim.AdamW(\n",
    "\t\t\tmodel.parameters(), lr=max_lr, weight_decay=weight_decay\n",
    "\t\t)\n",
    "\t\tsched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "\t\t\toptimizer, max_lr, epochs=1, steps_per_epoch=len(train_loader)\n",
    "\t\t)\n",
    "\n",
    "\t\tprint(f\"Client {i} previous losses: {previous_losses[i]}\")\n",
    "\t\thistory = fit(\n",
    "\t\t\t1,\n",
    "\t\t\tmodel,\n",
    "\t\t\ttqdm(train_loader, desc=f\"Client {i} Training\", colour=\"blue\"),\n",
    "\t\t\tval_loader,\n",
    "\t\t\tcriterion,\n",
    "\t\t\toptimizer,\n",
    "\t\t\tsched,\n",
    "\t\t)\n",
    "\t\tepoch_histories.append(history)\n",
    "\n",
    "\t\tprevious_losses[i].append(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"train_loss\": history[\"train_loss\"][-1],\n",
    "\t\t\t\t\"val_loss\": history[\"val_loss\"][-1],\n",
    "\t\t\t\t\"train_acc\": history[\"train_acc\"][-1],\n",
    "\t\t\t\t\"val_acc\": history[\"val_acc\"][-1],\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\t\tprint(f\"Client {i} updated losses: {previous_losses[i]}\")\n",
    "\n",
    "\t\twtarray = get_weights(model)\n",
    "\t\tcwts.append(encrypt_wt(wtarray, i))\n",
    "\t\tprint(f\"Client {i} weights encrypted for epoch {e+1}\")\n",
    "\n",
    "\thistories.append(epoch_histories)\n",
    "\tprint(f\"Epoch {e+1} completed\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize dictionaries to store accuracies, losses, and mIoU for each client\n",
    "train_accuracies = {i: [] for i in range(no_clients)}\n",
    "val_accuracies = {i: [] for i in range(no_clients)}\n",
    "train_losses = {i: [] for i in range(no_clients)}\n",
    "val_losses = {i: [] for i in range(no_clients)}\n",
    "train_miou = {i: [] for i in range(no_clients)}\n",
    "val_miou = {i: [] for i in range(no_clients)}\n",
    "\n",
    "# Populate the dictionaries with data from histories\n",
    "for epoch_histories in histories:\n",
    "\tfor i, history in enumerate(epoch_histories):\n",
    "\t\ttrain_accuracies[i].append(history[\"train_acc\"][-1])\n",
    "\t\tval_accuracies[i].append(history[\"val_acc\"][-1])\n",
    "\t\ttrain_losses[i].append(history[\"train_loss\"][-1])\n",
    "\t\tval_losses[i].append(history[\"val_loss\"][-1])\n",
    "\t\ttrain_miou[i].append(history[\"train_miou\"][-1])\n",
    "\t\tval_miou[i].append(history[\"val_miou\"][-1])\n",
    "\n",
    "# Plotting training accuracy for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_accuracies[i], label=f\"Client {i} Train Accuracy\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.title(f\"Client {i} Training Accuracy Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting validation accuracy for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(val_accuracies[i], label=f\"Client {i} Val Accuracy\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.title(f\"Client {i} Validation Accuracy Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting training loss for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_losses[i], label=f\"Client {i} Train Loss\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.title(f\"Client {i} Training Loss Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting validation loss for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(val_losses[i], label=f\"Client {i} Val Loss\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.title(f\"Client {i} Validation Loss Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting mean IoU for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_miou[i], label=f\"Client {i} Train mIoU\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Mean IoU\")\n",
    "\tplt.title(f\"Client {i} Training Mean IoU Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting all clients together for training accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(no_clients):\n",
    "\tplt.plot(train_accuracies[i], label=f\"Client {i} Train Accuracy\")\n",
    "plt.xlabel(\"Aggregation Round\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy Over Aggregation Rounds for All Clients\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the detailed history for each client and each aggregation round\n",
    "for e, epoch_histories in enumerate(histories):\n",
    "\tprint(f\"Aggregation Round {e+1} histories:\")\n",
    "\tfor i, history in enumerate(epoch_histories):\n",
    "\t\tprint(f\"  Client {i}: {history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneTestDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, img_path, mask_path, X, transform=None):\n",
    "\t\tself.img_path = img_path\n",
    "\t\tself.mask_path = mask_path\n",
    "\t\tself.X = X\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_full_path = os.path.join(self.img_path, self.X[idx] + \".jpg\")\n",
    "\t\tmask_full_path = os.path.join(self.mask_path, self.X[idx] + \".png\")\n",
    "\n",
    "\t\timg = cv2.imread(img_full_path)\n",
    "\t\tif img is None:\n",
    "\t\t\traise FileNotFoundError(f\"Image not found at {img_full_path}\")\n",
    "\n",
    "\t\tmask = cv2.imread(mask_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "\t\tif mask is None:\n",
    "\t\t\traise FileNotFoundError(f\"Mask not found at {mask_full_path}\")\n",
    "\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\taug = self.transform(image=img, mask=mask)\n",
    "\t\t\timg = Image.fromarray(aug[\"image\"])\n",
    "\t\t\tmask = aug[\"mask\"]\n",
    "\n",
    "\t\tif self.transform is None:\n",
    "\t\t\timg = Image.fromarray(img)\n",
    "\n",
    "\t\tmask = torch.from_numpy(mask).long()\n",
    "\n",
    "\t\treturn img, mask\n",
    "\n",
    "\n",
    "t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
    "test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(\n",
    "\tmodel, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "\tmodel.eval()\n",
    "\tt = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\timage = t(image)\n",
    "\tmodel.to(device)\n",
    "\timage = image.to(device)\n",
    "\tmask = mask.to(device)\n",
    "\twith torch.no_grad():\n",
    "\n",
    "\t\timage = image.unsqueeze(0)\n",
    "\t\tmask = mask.unsqueeze(0)\n",
    "\n",
    "\t\toutput = model(image)\n",
    "\t\tscore = mIoU(output, mask)\n",
    "\t\tmasked = torch.argmax(output, dim=1)\n",
    "\t\tmasked = masked.cpu().squeeze(0)\n",
    "\treturn masked, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_mask_pixel(\n",
    "\tmodel, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "\tmodel.eval()\n",
    "\tt = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\timage = t(image)\n",
    "\tmodel.to(device)\n",
    "\timage = image.to(device)\n",
    "\tmask = mask.to(device)\n",
    "\twith torch.no_grad():\n",
    "\n",
    "\t\timage = image.unsqueeze(0)\n",
    "\t\tmask = mask.unsqueeze(0)\n",
    "\n",
    "\t\toutput = model(image)\n",
    "\t\tacc = pixel_accuracy(output, mask)\n",
    "\t\tmasked = torch.argmax(output, dim=1)\n",
    "\t\tmasked = masked.cpu().squeeze(0)\n",
    "\treturn masked, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = test_set[3]\n",
    "pred_mask, score = predict_image_mask_miou(model, image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miou_score(model, test_set):\n",
    "\tscore_iou = []\n",
    "\tfor i in tqdm(range(len(test_set))):\n",
    "\t\timg, mask = test_set[i]\n",
    "\t\tpred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "\t\tscore_iou.append(score)\n",
    "\treturn score_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_miou = miou_score(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_acc(model, test_set):\n",
    "\taccuracy = []\n",
    "\tfor i in tqdm(range(len(test_set))):\n",
    "\t\timg, mask = test_set[i]\n",
    "\t\tpred_mask, acc = predict_image_mask_pixel(model, img, mask)\n",
    "\t\taccuracy.append(acc)\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_acc = pixel_acc(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "ax1.imshow(image)\n",
    "ax1.set_title(\"Picture\")\n",
    "\n",
    "ax2.imshow(mask)\n",
    "ax2.set_title(\"Ground truth\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3.imshow(pred_mask)\n",
    "ax3.set_title(\"UNet-MobileNet | mIoU {:.3f}\".format(score))\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3, mask3 = test_set[6]\n",
    "pred_mask3, score3 = predict_image_mask_miou(model, image3, mask3)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "ax1.imshow(image3)\n",
    "ax1.set_title(\"Picture\")\n",
    "\n",
    "ax2.imshow(mask3)\n",
    "ax2.set_title(\"Ground truth\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3.imshow(pred_mask3)\n",
    "ax3.set_title(\"UNet-MobileNet | mIoU {:.3f}\".format(score3))\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set mIoU\", np.mean(mob_miou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Pixel Accuracy\", np.mean(mob_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs-lab-12/.anaconda3/envs/he39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "from Pyfhel import Pyfhel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [0 , 1 ,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "HE = Pyfhel()\n",
    "ckks_params = {\n",
    "    \"scheme\": \"CKKS\",\n",
    "    \"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "    \"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "    \"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "}\n",
    "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "HE.rotateKeyGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffie_hellman_parameters():\n",
    "    parameters = dh.generate_parameters(generator=2, key_size=512)\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def generate_diffie_hellman_keys(parameters):\n",
    "    private_key = parameters.generate_private_key()\n",
    "    public_key = private_key.public_key()\n",
    "    return private_key, public_key\n",
    "\n",
    "\n",
    "def derive_key(private_key, peer_public_key):\n",
    "    shared_key = private_key.exchange(peer_public_key)\n",
    "    derived_key = HKDF(\n",
    "        algorithm=hashes.SHA256(),\n",
    "        length=32,\n",
    "        salt=None,\n",
    "        info=b\"handshake data\",\n",
    "    ).derive(shared_key)\n",
    "    return derived_key\n",
    "\n",
    "\n",
    "def encrypt_message_AES(key, message):\n",
    "    serialized_obj = pickle.dumps(message)\n",
    "    cipher = Cipher(algorithms.AES(key), modes.ECB())\n",
    "    encryptor = cipher.encryptor()\n",
    "    padded_obj = serialized_obj + b\" \" * (16 - len(serialized_obj) % 16)\n",
    "    ciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "    return ciphertext\n",
    "\n",
    "\n",
    "def decrypt_message_AES(key, ciphertext):\n",
    "    cipher = Cipher(algorithms.AES(key), modes.ECB())\n",
    "    decryptor = cipher.decryptor()\n",
    "    padded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "    serialized_obj = padded_obj.rstrip(b\" \")\n",
    "    obj = pickle.loads(serialized_obj)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def setup_AES():\n",
    "    num_clients = len(clients)\n",
    "    parameters = generate_diffie_hellman_parameters()\n",
    "    server_private_key, server_public_key = generate_diffie_hellman_keys(parameters)\n",
    "    client_keys = [generate_diffie_hellman_keys(parameters) for _ in range(num_clients)]\n",
    "    shared_keys = [\n",
    "        derive_key(server_private_key, client_public_key)\n",
    "        for _, client_public_key in client_keys\n",
    "    ]\n",
    "    client_shared_keys = [\n",
    "        derive_key(client_private_key, server_public_key)\n",
    "        for client_private_key, _ in client_keys\n",
    "    ]\n",
    "\n",
    "    return client_keys, shared_keys, client_shared_keys\n",
    "\n",
    "\n",
    "client_keys, shared_keys, client_shared_keys = setup_AES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights):\n",
    "    with torch.no_grad():\n",
    "        for param, weight in zip(model.parameters(), weights):\n",
    "            param.copy_(torch.tensor(weight))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model):\n",
    "    return [param.cpu().detach().numpy() for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wt(encypted_cwts):\n",
    "    cwts = []\n",
    "    for i, ecwt in enumerate(encypted_cwts):\n",
    "        cwts.append(decrypt_message_AES(shared_keys[0], ecwt))\n",
    "    # cwts = encypted_cwts\n",
    "    resmodel = []\n",
    "    for j in range(len(cwts[0])):  # for layers\n",
    "        layer = []\n",
    "        for k in range(len(cwts[0][j])):  # for chunks\n",
    "            tmp = cwts[0][j][k].copy()\n",
    "            for i in range(1, len(cwts)):  # for clients\n",
    "                tmp = tmp + cwts[i][j][k]\n",
    "            tmp = tmp / len(cwts)\n",
    "            layer.append(tmp)\n",
    "        resmodel.append(layer)\n",
    "\n",
    "    res = [resmodel.copy() for _ in range(len(clients))]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_wt(wtarray, i):\n",
    "    cwt = []\n",
    "    for layer in wtarray:\n",
    "        flat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "        chunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "        clayer = []\n",
    "        for chunk in chunks:\n",
    "            ptxt = HE.encodeFrac(chunk)\n",
    "            ctxt = HE.encryptPtxt(ptxt)\n",
    "            clayer.append(ctxt)\n",
    "        cwt.append(clayer.copy())\n",
    "    ciphertext = encrypt_message_AES(client_shared_keys[i], cwt)\n",
    "    return ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_weights(res):\n",
    "    decrypted_weights = []\n",
    "    for client_weights, model in zip(res, models):\n",
    "        decrypted_client_weights = []\n",
    "        wtarray = get_weights(model)\n",
    "        for layer_weights, layer in zip(client_weights, wtarray):\n",
    "            decrypted_layer_weights = []\n",
    "            flat_array = layer.astype(np.float64).flatten()\n",
    "            chunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "            for chunk, encrypted_chunk in zip(chunks, layer_weights):\n",
    "                decrypted_chunk = HE.decryptFrac(encrypted_chunk)\n",
    "                original_chunk_size = len(chunk)\n",
    "                decrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "                decrypted_layer_weights.append(decrypted_chunk)\n",
    "            decrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
    "            decrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
    "            decrypted_client_weights.append(decrypted_layer_weights)\n",
    "        decrypted_weights.append(decrypted_client_weights)\n",
    "    return decrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading GPT-2 Small model...\n",
      "Model loaded successfully!\n",
      "Preparing dummy text dataset...\n",
      "Dummy text dataset prepared successfully!\n",
      "Training model for one epoch without encryption...\n",
      "[10] loss: 3.940\n",
      "Training time without encryption: 3.41 seconds\n",
      "Encrypting model parameters...\n",
      "Encryption time: 0.00 seconds\n",
      "Updating model with encrypted parameters...\n",
      "Model updated with encrypted parameters!\n",
      "Training model for one epoch with encrypted parameters...\n",
      "[10] loss: 10.617\n",
      "Training time with encryption: 1.77 seconds\n",
      "Final Results:\n",
      "Training time without encryption: 3.41 seconds\n",
      "Training time with encryption: 1.77 seconds\n",
      "Encryption time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Example encryption function (dummy encryption)\n",
    "def encrypt_model_parameters(parameters):\n",
    "    encrypted_params = [param**2 for param in parameters]\n",
    "    return encrypted_params\n",
    "\n",
    "\n",
    "# Define a simple text dataset\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize the GPT-2 Small model and tokenizer\n",
    "print(\"Loading GPT-2 Small model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Prepare dummy text dataset\n",
    "print(\"Preparing dummy text dataset...\")\n",
    "texts = [\"This is a sample text for GPT-2 Small.\"] * 50\n",
    "dataset = SimpleTextDataset(texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "print(\"Dummy text dataset prepared successfully!\")\n",
    "\n",
    "\n",
    "# Function to train the model for one epoch\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"[{i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "# Measure training time without encryption\n",
    "print(\"Training model for one epoch without encryption...\")\n",
    "start_time = time.time()\n",
    "train_one_epoch(model, dataloader, criterion, optimizer, device)\n",
    "end_time = time.time()\n",
    "time_without_encryption = end_time - start_time\n",
    "print(f\"Training time without encryption: {time_without_encryption:.2f} seconds\")\n",
    "\n",
    "# Encrypt model parameters\n",
    "print(\"Encrypting model parameters...\")\n",
    "model_parameters = [param.data.clone().detach() for param in model.parameters()]\n",
    "start_encryption_time = time.time()\n",
    "encrypted_parameters = encrypt_model_parameters(model_parameters)\n",
    "end_encryption_time = time.time()\n",
    "encryption_time = end_encryption_time - start_encryption_time\n",
    "print(f\"Encryption time: {encryption_time:.2f} seconds\")\n",
    "\n",
    "# Update model with encrypted parameters (dummy update for demonstration)\n",
    "print(\"Updating model with encrypted parameters...\")\n",
    "for param, encrypted_param in zip(model.parameters(), encrypted_parameters):\n",
    "    param.data = encrypted_param.to(device)\n",
    "print(\"Model updated with encrypted parameters!\")\n",
    "\n",
    "# Measure training time with encryption\n",
    "print(\"Training model for one epoch with encrypted parameters...\")\n",
    "start_time = time.time()\n",
    "train_one_epoch(model, dataloader, criterion, optimizer, device)\n",
    "end_time = time.time()\n",
    "time_with_encryption = end_time - start_time\n",
    "print(f\"Training time with encryption: {time_with_encryption:.2f} seconds\")\n",
    "\n",
    "# Print final results\n",
    "print(\"Final Results:\")\n",
    "print(f\"Training time without encryption: {time_without_encryption:.2f} seconds\")\n",
    "print(f\"Training time with encryption: {time_with_encryption:.2f} seconds\")\n",
    "print(f\"Encryption time: {encryption_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " encryption time : 703.87 seconds\n"
     ]
    }
   ],
   "source": [
    "start_encryption_time = time.time()\n",
    "ew = encrypt_wt(get_weights(model), 0)\n",
    "end_encryption_time = time.time()\n",
    "encryption_time = end_encryption_time - start_encryption_time\n",
    "print(f\" encryption time : {encryption_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aggregation time : 7264.64 seconds\n"
     ]
    }
   ],
   "source": [
    "ags_time = time.time()\n",
    "res = aggregate_wt([ew for _ in range(10)])\n",
    "age_time = time.time()  \n",
    "print(f\" aggregation time : {age_time - ags_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dencryption time : 994.67 seconds\n"
     ]
    }
   ],
   "source": [
    "start_dencryption_time = time.time()\n",
    "ew = encrypt_wt(get_weights(model), 0)\n",
    "end_encryption_time = time.time()\n",
    "encryption_time = end_encryption_time - start_dencryption_time\n",
    "print(f\" dencryption time : {encryption_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in      : 124439808\n",
      "Size of the       model: 474.75 MB\n",
      "Final Results:\n",
      "Number of parameters in : 124439808\n",
      "Size of the : 474.75 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "\n",
    "\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in      : {num_params}\")\n",
    "\n",
    "# Save the model to disk and get its size\n",
    "model_path = \"resnet50.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "model_size = os.path.getsize(model_path)\n",
    "print(f\"Size of the       model: {model_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Cleanup\n",
    "os.remove(model_path)\n",
    "\n",
    "# Print results\n",
    "print(f\"Final Results:\")\n",
    "print(f\"Number of parameters in : {num_params}\")\n",
    "print(f\"Size of the : {model_size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16018104177\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "size = sys.getsizeof(ew)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of pickle file: 510421743 bytes\n",
      "Size of pickle file: 498458.73 KB\n",
      "Size of pickle file: 486.78 MB\n",
      "Size of pickle file: 0.475367 GB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "example_obj = model\n",
    "\n",
    "# Save object to a pickle file\n",
    "pickle_file = \"example_obj.pkl\"\n",
    "with open(pickle_file, \"wb\") as file:\n",
    "    pickle.dump(example_obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Check the size of the pickle file\n",
    "file_size_bytes = os.path.getsize(pickle_file)\n",
    "file_size_kb = file_size_bytes / 1024\n",
    "file_size_mb = file_size_kb / 1024\n",
    "file_size_gb = file_size_mb / 1024\n",
    "\n",
    "print(f\"Size of pickle file: {file_size_bytes} bytes\")\n",
    "print(f\"Size of pickle file: {file_size_kb:.2f} KB\")\n",
    "print(f\"Size of pickle file: {file_size_mb:.2f} MB\")\n",
    "print(f\"Size of pickle file: {file_size_gb:.6f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of pickle file: 16018104157 bytes\n",
      "Size of pickle file: 15642679.84 KB\n",
      "Size of pickle file: 15276.05 MB\n",
      "Size of pickle file: 14.918022 GB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "example_obj = ew\n",
    "\n",
    "# Save object to a pickle file\n",
    "pickle_file = \"example_obj.pkl\"\n",
    "with open(pickle_file, \"wb\") as file:\n",
    "    pickle.dump(example_obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Check the size of the pickle file\n",
    "file_size_bytes = os.path.getsize(pickle_file)\n",
    "file_size_kb = file_size_bytes / 1024\n",
    "file_size_mb = file_size_kb / 1024\n",
    "file_size_gb = file_size_mb / 1024\n",
    "\n",
    "print(f\"Size of pickle file: {file_size_bytes} bytes\")\n",
    "print(f\"Size of pickle file: {file_size_kb:.2f} KB\")\n",
    "print(f\"Size of pickle file: {file_size_mb:.2f} MB\")\n",
    "print(f\"Size of pickle file: {file_size_gb:.6f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

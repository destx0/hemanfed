{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.323880\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.116900\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.053796\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.109084\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.029141\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.015517\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.033405\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.169364\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.071979\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.017166\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9843/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.016116\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.068912\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.096089\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011658\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.087023\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.113086\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.005595\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.111478\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.105128\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.040727\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9867/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.022155\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.007778\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.008289\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.015772\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.010310\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.035779\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.008721\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.008064\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.013669\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.084111\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001537\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.004607\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.019117\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.022603\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.000384\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.073754\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.056858\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007931\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001583\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.075571\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.015460\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.011518\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004636\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.003645\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.039488\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.001828\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000247\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.011437\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.002185\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.035736\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.081813\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.002984\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.039004\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005565\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000213\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.012224\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.040270\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.001098\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.024789\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.005501\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000330\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001103\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.005143\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000632\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.000067\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.000474\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.070494\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001754\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.002171\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.007143\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005829\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.000086\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.007707\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002230\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Running the training and testing loops\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     test(model, test_loader, criterion)\n",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, train_loader, criterion, optimizer, epoch):\n\u001b[1;32m     52\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     54\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     55\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/he39/lib/python3.9/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for the MNIST dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Running the training and testing loops\n",
    "for epoch in range(1, 11):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Client 1 Accuracy: 0.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Client 2 Accuracy: 0.7956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Client 3 Accuracy: 0.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. inf >> 13.196\n",
      "Epoch:1/10.. Train Loss: 0.006.. Val Loss: 13.196.. Train Acc:0.890.. Val Acc:0.608.. Time: 0.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Client 1 Accuracy: 0.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Client 2 Accuracy: 0.6938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Client 3 Accuracy: 0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Not Decrease for 1 time\n",
      "Epoch:2/10.. Train Loss: 1.358.. Val Loss: 65.187.. Train Acc:0.873.. Val Acc:0.197.. Time: 0.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Client 1 Accuracy: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Client 2 Accuracy: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 21.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Client 3 Accuracy: 0.9587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 65.187 >> 50.324\n",
      "Epoch:3/10.. Train Loss: 0.005.. Val Loss: 50.324.. Train Acc:0.958.. Val Acc:0.348.. Time: 0.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Client 1 Accuracy: 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Client 2 Accuracy: 0.9173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Client 3 Accuracy: 0.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Not Decrease for 2 time\n",
      "Epoch:4/10.. Train Loss: 0.006.. Val Loss: 69.231.. Train Acc:0.949.. Val Acc:0.361.. Time: 0.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Client 1 Accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Client 2 Accuracy: 0.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 21.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Client 3 Accuracy: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Not Decrease for 3 time\n",
      "Epoch:5/10.. Train Loss: 0.007.. Val Loss: 94.983.. Train Acc:0.954.. Val Acc:0.404.. Time: 0.60m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 21.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Client 1 Accuracy: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Client 2 Accuracy: 0.7367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Client 3 Accuracy: 0.8993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Not Decrease for 4 time\n",
      "Epoch:6/10.. Train Loss: 0.132.. Val Loss: 211.805.. Train Acc:0.827.. Val Acc:0.196.. Time: 0.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Client 1 Accuracy: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Client 2 Accuracy: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Client 3 Accuracy: 0.5614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Not Decrease for 5 time\n",
      "Epoch:7/10.. Train Loss: 0.169.. Val Loss: 406.192.. Train Acc:0.814.. Val Acc:0.137.. Time: 0.58m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Client 1 Accuracy: 0.7971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Client 2 Accuracy: 0.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Client 3 Accuracy: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 406.192 >> 207.780\n",
      "Epoch:8/10.. Train Loss: 0.025.. Val Loss: 207.780.. Train Acc:0.847.. Val Acc:0.161.. Time: 0.58m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Client 1 Accuracy: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Client 2 Accuracy: 0.4847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Client 3 Accuracy: 0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Not Decrease for 6 time\n",
      "Epoch:9/10.. Train Loss: 243800726645.141.. Val Loss: 5209.912.. Train Acc:0.622.. Val Acc:0.107.. Time: 0.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Client 1 Accuracy: 0.1307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:11<00:00, 22.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Client 2 Accuracy: 0.1390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Client 3 Accuracy: 0.1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 5209.912 >> 1681.246\n",
      "saving model...\n",
      "Epoch:10/10.. Train Loss: 8884541.179.. Val Loss: 1681.246.. Train Acc:0.131.. Val Acc:0.107.. Time: 0.58m\n",
      "Total time: 5.88 m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def split_dataset_by_classes(dataset, n_clients):\n",
    "    class_indices = [[] for _ in range(10)]\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    for class_idx in range(10):\n",
    "        for client_idx in range(n_clients):\n",
    "            client_indices[client_idx].extend(\n",
    "                class_indices[class_idx][client_idx::n_clients]\n",
    "            )\n",
    "\n",
    "    client_datasets = [Subset(dataset, indices) for indices in client_indices]\n",
    "    return client_datasets\n",
    "\n",
    "\n",
    "def average_weights(models):\n",
    "    avg_model = copy.deepcopy(models[0])\n",
    "    for key in avg_model.state_dict().keys():\n",
    "        avg_model.state_dict()[key] = torch.mean(\n",
    "            torch.stack([model.state_dict()[key] for model in models]), dim=0\n",
    "        )\n",
    "    return avg_model\n",
    "\n",
    "\n",
    "def federated_fit(\n",
    "    epochs,\n",
    "    model,\n",
    "    client_loaders,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    adaptation_steps=5,\n",
    "    inner_lr=0.01,\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        overall_accuracy = 0\n",
    "\n",
    "        model.train()\n",
    "        client_accuracies = []\n",
    "        client_models = []\n",
    "        for client_idx, client_loader in enumerate(client_loaders):\n",
    "            client_model = copy.deepcopy(model)\n",
    "            client_model.to(device)\n",
    "            client_optimizer = optim.Adam(\n",
    "                client_model.parameters(), lr=inner_lr, weight_decay=1e-5\n",
    "            )\n",
    "            client_accuracy = 0\n",
    "            for i, data in enumerate(tqdm(client_loader)):\n",
    "                images, labels = data\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                client_optimizer.zero_grad()\n",
    "\n",
    "                learner = l2l.algorithms.MAML(client_model, lr=inner_lr).clone()\n",
    "\n",
    "                for step in range(adaptation_steps):\n",
    "                    output = learner(images)\n",
    "                    loss = criterion(output, labels)\n",
    "                    learner.adapt(loss)\n",
    "\n",
    "                output = learner(images)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                client_optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                client_accuracy += (\n",
    "                    (output.argmax(dim=1) == labels).float().mean().item()\n",
    "                )\n",
    "\n",
    "            client_accuracy /= len(client_loader)\n",
    "            client_accuracies.append(client_accuracy)\n",
    "            client_models.append(client_model)\n",
    "            print(\n",
    "                f\"Epoch {e + 1}, Client {client_idx + 1} Accuracy: {client_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Aggregate the client models' weights\n",
    "        model = average_weights(client_models)\n",
    "        model.to(device)\n",
    "\n",
    "        overall_accuracy = sum(client_accuracies) / len(client_accuracies)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(val_loader)):\n",
    "                images, labels = data\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = model(images)\n",
    "                test_accuracy += (output.argmax(dim=1) == labels).float().mean().item()\n",
    "                loss = criterion(output, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        train_losses.append(\n",
    "            running_loss / sum(len(loader.dataset) for loader in client_loaders)\n",
    "        )\n",
    "        val_losses.append(test_loss / len(val_loader))\n",
    "        val_accuracy = test_accuracy / len(val_loader)\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "        if min_loss > (test_loss / len(val_loader)):\n",
    "            print(\n",
    "                \"Loss Decreasing.. {:.3f} >> {:.3f}\".format(\n",
    "                    min_loss, (test_loss / len(val_loader))\n",
    "                )\n",
    "            )\n",
    "            min_loss = test_loss / len(val_loader)\n",
    "            decrease += 1\n",
    "            if decrease % 5 == 0:\n",
    "                print(\"saving model...\")\n",
    "                torch.save(\n",
    "                    model,\n",
    "                    \"Federated-MAML-Model-Accuracy-{:.3f}.pt\".format(val_accuracy),\n",
    "                )\n",
    "\n",
    "        if (test_loss / len(val_loader)) > min_loss:\n",
    "            not_improve += 1\n",
    "            min_loss = test_loss / len(val_loader)\n",
    "            print(f\"Loss Not Decrease for {not_improve} time\")\n",
    "            if not_improve == 7:\n",
    "                print(\"Loss not decrease for 7 times, Stop Training\")\n",
    "                break\n",
    "\n",
    "        train_acc.append(overall_accuracy)\n",
    "        print(\n",
    "            \"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "            \"Train Loss: {:.3f}..\".format(\n",
    "                running_loss / sum(len(loader.dataset) for loader in client_loaders)\n",
    "            ),\n",
    "            \"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "            \"Train Acc:{:.3f}..\".format(overall_accuracy),\n",
    "            \"Val Acc:{:.3f}..\".format(val_accuracy),\n",
    "            \"Time: {:.2f}m\".format((time.time() - since) / 60),\n",
    "        )\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"lrs\": lrs,\n",
    "    }\n",
    "    print(\"Total time: {:.2f} m\".format((time.time() - fit_time) / 60))\n",
    "    return history\n",
    "\n",
    "\n",
    "n_clients = 3\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "full_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "client_datasets = split_dataset_by_classes(train_dataset, n_clients)\n",
    "client_loaders = [\n",
    "    DataLoader(dataset, batch_size=64, shuffle=True) for dataset in client_datasets\n",
    "]\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = federated_fit(\n",
    "    epochs=10,\n",
    "    model=model,\n",
    "    client_loaders=client_loaders,\n",
    "    val_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    adaptation_steps=5,\n",
    "    inner_lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "current_timestamp = datetime.now()\n",
    "\n",
    "# Format the timestamp in a human-readable form\n",
    "folder_path = current_timestamp.strftime(\"%d_%H_%M\")\n",
    "fp = f\"models/{folder_path}\"\n",
    "if not os.path.exists(fp):\n",
    "\tos.makedirs(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [0 , 1 , 2]\n",
    "no_clients = len(clients)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lr = 1e-3\n",
    "# epoch = 2\n",
    "# weight_decay = 1e-4\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer, max_lr, epochs=epoch, steps_per_epoch=len(train_loader)\n",
    "# )\n",
    "\n",
    "# history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "HE = Pyfhel()\n",
    "ckks_params = {\n",
    "\t\"scheme\": \"CKKS\",\n",
    "\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "}\n",
    "HE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "HE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "HE.rotateKeyGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffie_hellman_parameters():\n",
    "\tparameters = dh.generate_parameters(generator=2, key_size=512)\n",
    "\treturn parameters\n",
    "\n",
    "\n",
    "def generate_diffie_hellman_keys(parameters):\n",
    "\tprivate_key = parameters.generate_private_key()\n",
    "\tpublic_key = private_key.public_key()\n",
    "\treturn private_key, public_key\n",
    "\n",
    "\n",
    "def derive_key(private_key, peer_public_key):\n",
    "\tshared_key = private_key.exchange(peer_public_key)\n",
    "\tderived_key = HKDF(\n",
    "\t\talgorithm=hashes.SHA256(),\n",
    "\t\tlength=32,\n",
    "\t\tsalt=None,\n",
    "\t\tinfo=b\"handshake data\",\n",
    "\t).derive(shared_key)\n",
    "\treturn derived_key\n",
    "\n",
    "\n",
    "def encrypt_message_AES(key, message):\n",
    "\tserialized_obj = pickle.dumps(message)\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.ECB())\n",
    "\tencryptor = cipher.encryptor()\n",
    "\tpadded_obj = serialized_obj + b\" \" * (16 - len(serialized_obj) % 16)\n",
    "\tciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "\treturn ciphertext\n",
    "\n",
    "\n",
    "def decrypt_message_AES(key, ciphertext):\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.ECB())\n",
    "\tdecryptor = cipher.decryptor()\n",
    "\tpadded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "\tserialized_obj = padded_obj.rstrip(b\" \")\n",
    "\tobj = pickle.loads(serialized_obj)\n",
    "\treturn obj\n",
    "\n",
    "\n",
    "def setup_AES():\n",
    "\tnum_clients = len(clients)\n",
    "\tparameters = generate_diffie_hellman_parameters()\n",
    "\tserver_private_key, server_public_key = generate_diffie_hellman_keys(parameters)\n",
    "\tclient_keys = [generate_diffie_hellman_keys(parameters) for _ in range(num_clients)]\n",
    "\tshared_keys = [\n",
    "\t\tderive_key(server_private_key, client_public_key)\n",
    "\t\tfor _, client_public_key in client_keys\n",
    "\t]\n",
    "\tclient_shared_keys = [\n",
    "\t\tderive_key(client_private_key, server_public_key)\n",
    "\t\tfor client_private_key, _ in client_keys\n",
    "\t]\n",
    "\n",
    "\treturn client_keys, shared_keys, client_shared_keys\n",
    "\n",
    "client_keys, shared_keys, client_shared_keys = setup_AES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights):\n",
    "\twith torch.no_grad(): \n",
    "\t\tfor param, weight in zip(model.parameters(), weights):\n",
    "\t\t\tparam.copy_(torch.tensor(weight))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model):\n",
    "\treturn [param.cpu().detach().numpy() for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wt(encypted_cwts):\n",
    "\t# cwts = []\n",
    "\t# for i, ecwt in enumerate(encypted_cwts):\n",
    "\t# \tcwts.append(decrypt_message_AES(shared_keys[i], ecwt))\n",
    "\tcwts = encypted_cwts\n",
    "\tresmodel = []\n",
    "\tfor j in range(len(cwts[0])):  # for layers\n",
    "\t\tlayer = []\n",
    "\t\tfor k in range(len(cwts[0][j])):  # for chunks\n",
    "\t\t\ttmp = cwts[0][j][k].copy()\n",
    "\t\t\tfor i in range(1, len(cwts)):  # for clients\n",
    "\t\t\t\ttmp = tmp + cwts[i][j][k]\n",
    "\t\t\ttmp = tmp / len(cwts)\n",
    "\t\t\tlayer.append(tmp)\n",
    "\t\tresmodel.append(layer)\n",
    "\n",
    "\tres = [resmodel.copy() for _ in range(len(clients))]\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_wt(wtarray, i):\n",
    "\tcwt = []\n",
    "\tfor layer in wtarray:\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\t\tclayer = []\n",
    "\t\tfor chunk in chunks:\n",
    "\t\t\tptxt = HE.encodeFrac(chunk)\n",
    "\t\t\tctxt = HE.encryptPtxt(ptxt)\n",
    "\t\t\tclayer.append(ctxt)\n",
    "\t\tcwt.append(clayer.copy())\n",
    "\t# ciphertext = encrypt_message_AES(client_shared_keys[i], cwt)\n",
    "\t# return ciphertext\n",
    "\treturn cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_weights(res):\n",
    "\tdecrypted_weights = []\n",
    "\tfor client_weights, model in zip(res, models):\n",
    "\t\tdecrypted_client_weights = []\n",
    "\t\twtarray = get_weights(model)\n",
    "\t\tfor layer_weights, layer in zip(client_weights, wtarray):\n",
    "\t\t\tdecrypted_layer_weights = []\n",
    "\t\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\t\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\t\t\tfor chunk, encrypted_chunk in zip(chunks, layer_weights):\n",
    "\t\t\t\tdecrypted_chunk = HE.decryptFrac(encrypted_chunk)\n",
    "\t\t\t\toriginal_chunk_size = len(chunk)\n",
    "\t\t\t\tdecrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "\t\t\t\tdecrypted_layer_weights.append(decrypted_chunk)\n",
    "\t\t\tdecrypted_layer_weights = np.concatenate(decrypted_layer_weights, axis=0)\n",
    "\t\t\tdecrypted_layer_weights = decrypted_layer_weights.reshape(layer.shape)\n",
    "\t\t\tdecrypted_client_weights.append(decrypted_layer_weights)\n",
    "\t\tdecrypted_weights.append(decrypted_client_weights)\n",
    "\treturn decrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 0.01\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "previous_losses = {i: [] for i in range(no_clients)}\n",
    "\n",
    "cwts = [encrypt_wt(get_weights(model), i) for i, model in enumerate(models)]\n",
    "print(\"Initial encrypted weights generated for all clients.\")\n",
    "\n",
    "for e in tqdm(range(epochs), desc=\"Epochs\", colour=\"green\"):\n",
    "\tprint(f\"Epoch {e+1}/{epochs} started\")\n",
    "\tcwts = aggregate_wt(cwts)\n",
    "\tprint(f\"Aggregated encrypted weights after epoch {e+1}\")\n",
    "\twts = decrypt_weights(cwts)\n",
    "\tprint(f\"Decrypted weights after aggregation for epoch {e+1}\")\n",
    "\n",
    "\tcwts = []\n",
    "\tepoch_histories = []\n",
    "\n",
    "\tfor i in range(no_clients):\n",
    "\t\tprint(f\"Client {i} preparing for epoch {e+1}\")\n",
    "\t\twt = wts[i]\n",
    "\t\tmodel = load_weights(models[i], wt)\n",
    "\t\tif (e % 5 == 0) and i == 0:\n",
    "\t\t\ttorch.save(model, f\"{fp}/{e}_model.pth\")\n",
    "\t\ttrain_loader = train_loaders[i]\n",
    "\t\tval_loader = val_loaders[i]\n",
    "\n",
    "\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\toptimizer = torch.optim.AdamW(\n",
    "\t\t\tmodel.parameters(), lr=max_lr, weight_decay=weight_decay\n",
    "\t\t)\n",
    "\t\tsched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "\t\t\toptimizer, max_lr, epochs=1, steps_per_epoch=len(train_loader)\n",
    "\t\t)\n",
    "\n",
    "\t\tprint(f\"Client {i} previous losses: {previous_losses[i]}\")\n",
    "\t\thistory = fit(\n",
    "\t\t\t1,\n",
    "\t\t\tmodel,\n",
    "\t\t\ttqdm(train_loader, desc=f\"Client {i} Training\", colour=\"blue\"),\n",
    "\t\t\tval_loader,\n",
    "\t\t\tcriterion,\n",
    "\t\t\toptimizer,\n",
    "\t\t\tsched,\n",
    "\t\t)\n",
    "\t\tepoch_histories.append(history)\n",
    "\n",
    "\t\tprevious_losses[i].append(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"train_loss\": history[\"train_loss\"][-1],\n",
    "\t\t\t\t\"val_loss\": history[\"val_loss\"][-1],\n",
    "\t\t\t\t\"train_acc\": history[\"train_acc\"][-1],\n",
    "\t\t\t\t\"val_acc\": history[\"val_acc\"][-1],\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\t\tprint(f\"Client {i} updated losses: {previous_losses[i]}\")\n",
    "\n",
    "\t\twtarray = get_weights(model)\n",
    "\t\tcwts.append(encrypt_wt(wtarray, i))\n",
    "\t\tprint(f\"Client {i} weights encrypted for epoch {e+1}\")\n",
    "\n",
    "\thistories.append(epoch_histories)\n",
    "\tprint(f\"Epoch {e+1} completed\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize dictionaries to store accuracies, losses, and mIoU for each client\n",
    "train_accuracies = {i: [] for i in range(no_clients)}\n",
    "val_accuracies = {i: [] for i in range(no_clients)}\n",
    "train_losses = {i: [] for i in range(no_clients)}\n",
    "val_losses = {i: [] for i in range(no_clients)}\n",
    "train_miou = {i: [] for i in range(no_clients)}\n",
    "val_miou = {i: [] for i in range(no_clients)}\n",
    "\n",
    "# Populate the dictionaries with data from histories\n",
    "for epoch_histories in histories:\n",
    "\tfor i, history in enumerate(epoch_histories):\n",
    "\t\ttrain_accuracies[i].append(history[\"train_acc\"][-1])\n",
    "\t\tval_accuracies[i].append(history[\"val_acc\"][-1])\n",
    "\t\ttrain_losses[i].append(history[\"train_loss\"][-1])\n",
    "\t\tval_losses[i].append(history[\"val_loss\"][-1])\n",
    "\t\ttrain_miou[i].append(history[\"train_miou\"][-1])\n",
    "\t\tval_miou[i].append(history[\"val_miou\"][-1])\n",
    "\n",
    "# Plotting training accuracy for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_accuracies[i], label=f\"Client {i} Train Accuracy\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.title(f\"Client {i} Training Accuracy Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting validation accuracy for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(val_accuracies[i], label=f\"Client {i} Val Accuracy\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.title(f\"Client {i} Validation Accuracy Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting training loss for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_losses[i], label=f\"Client {i} Train Loss\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.title(f\"Client {i} Training Loss Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting validation loss for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(val_losses[i], label=f\"Client {i} Val Loss\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.title(f\"Client {i} Validation Loss Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting mean IoU for each client independently\n",
    "for i in range(no_clients):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_miou[i], label=f\"Client {i} Train mIoU\")\n",
    "\tplt.xlabel(\"Aggregation Round\")\n",
    "\tplt.ylabel(\"Mean IoU\")\n",
    "\tplt.title(f\"Client {i} Training Mean IoU Over Aggregation Rounds\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting all clients together for training accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(no_clients):\n",
    "\tplt.plot(train_accuracies[i], label=f\"Client {i} Train Accuracy\")\n",
    "plt.xlabel(\"Aggregation Round\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy Over Aggregation Rounds for All Clients\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the detailed history for each client and each aggregation round\n",
    "for e, epoch_histories in enumerate(histories):\n",
    "\tprint(f\"Aggregation Round {e+1} histories:\")\n",
    "\tfor i, history in enumerate(epoch_histories):\n",
    "\t\tprint(f\"  Client {i}: {history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneTestDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, img_path, mask_path, X, transform=None):\n",
    "\t\tself.img_path = img_path\n",
    "\t\tself.mask_path = mask_path\n",
    "\t\tself.X = X\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_full_path = os.path.join(self.img_path, self.X[idx] + \".jpg\")\n",
    "\t\tmask_full_path = os.path.join(self.mask_path, self.X[idx] + \".png\")\n",
    "\n",
    "\t\timg = cv2.imread(img_full_path)\n",
    "\t\tif img is None:\n",
    "\t\t\traise FileNotFoundError(f\"Image not found at {img_full_path}\")\n",
    "\n",
    "\t\tmask = cv2.imread(mask_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "\t\tif mask is None:\n",
    "\t\t\traise FileNotFoundError(f\"Mask not found at {mask_full_path}\")\n",
    "\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\taug = self.transform(image=img, mask=mask)\n",
    "\t\t\timg = Image.fromarray(aug[\"image\"])\n",
    "\t\t\tmask = aug[\"mask\"]\n",
    "\n",
    "\t\tif self.transform is None:\n",
    "\t\t\timg = Image.fromarray(img)\n",
    "\n",
    "\t\tmask = torch.from_numpy(mask).long()\n",
    "\n",
    "\t\treturn img, mask\n",
    "\n",
    "\n",
    "t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
    "test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(\n",
    "\tmodel, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "\tmodel.eval()\n",
    "\tt = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\timage = t(image)\n",
    "\tmodel.to(device)\n",
    "\timage = image.to(device)\n",
    "\tmask = mask.to(device)\n",
    "\twith torch.no_grad():\n",
    "\n",
    "\t\timage = image.unsqueeze(0)\n",
    "\t\tmask = mask.unsqueeze(0)\n",
    "\n",
    "\t\toutput = model(image)\n",
    "\t\tscore = mIoU(output, mask)\n",
    "\t\tmasked = torch.argmax(output, dim=1)\n",
    "\t\tmasked = masked.cpu().squeeze(0)\n",
    "\treturn masked, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_mask_pixel(\n",
    "\tmodel, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "\tmodel.eval()\n",
    "\tt = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\timage = t(image)\n",
    "\tmodel.to(device)\n",
    "\timage = image.to(device)\n",
    "\tmask = mask.to(device)\n",
    "\twith torch.no_grad():\n",
    "\n",
    "\t\timage = image.unsqueeze(0)\n",
    "\t\tmask = mask.unsqueeze(0)\n",
    "\n",
    "\t\toutput = model(image)\n",
    "\t\tacc = pixel_accuracy(output, mask)\n",
    "\t\tmasked = torch.argmax(output, dim=1)\n",
    "\t\tmasked = masked.cpu().squeeze(0)\n",
    "\treturn masked, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = test_set[3]\n",
    "pred_mask, score = predict_image_mask_miou(model, image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miou_score(model, test_set):\n",
    "\tscore_iou = []\n",
    "\tfor i in tqdm(range(len(test_set))):\n",
    "\t\timg, mask = test_set[i]\n",
    "\t\tpred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "\t\tscore_iou.append(score)\n",
    "\treturn score_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_miou = miou_score(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_acc(model, test_set):\n",
    "\taccuracy = []\n",
    "\tfor i in tqdm(range(len(test_set))):\n",
    "\t\timg, mask = test_set[i]\n",
    "\t\tpred_mask, acc = predict_image_mask_pixel(model, img, mask)\n",
    "\t\taccuracy.append(acc)\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_acc = pixel_acc(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "ax1.imshow(image)\n",
    "ax1.set_title(\"Picture\")\n",
    "\n",
    "ax2.imshow(mask)\n",
    "ax2.set_title(\"Ground truth\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3.imshow(pred_mask)\n",
    "ax3.set_title(\"UNet-MobileNet | mIoU {:.3f}\".format(score))\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3, mask3 = test_set[6]\n",
    "pred_mask3, score3 = predict_image_mask_miou(model, image3, mask3)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "ax1.imshow(image3)\n",
    "ax1.set_title(\"Picture\")\n",
    "\n",
    "ax2.imshow(mask3)\n",
    "ax2.set_title(\"Ground truth\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3.imshow(pred_mask3)\n",
    "ax3.set_title(\"UNet-MobileNet | mIoU {:.3f}\".format(score3))\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set mIoU\", np.mean(mob_miou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Pixel Accuracy\", np.mean(mob_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
